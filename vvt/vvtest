#!/usr/bin/env python

# Copyright 2018 National Technology & Engineering Solutions of Sandia, LLC
# (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S.
# Government retains certain rights in this software.

import sys
sys.dont_write_bytecode = True
sys.excepthook = sys.__excepthook__
import os
import time
import shutil
import glob
import shlex
import string
import random

import libvvtest.cmdline as cmdline
import libvvtest.vvplatform as vvplatform
import libvvtest.misc as misc
import libvvtest.TestSpec as TestSpec
import libvvtest.TestExec as TestExec
import libvvtest.TestList as TestList
import libvvtest.batchutils as batchutils
import libvvtest.FilterExpressions as FilterExpressions
from libvvtest.RuntimeConfig import RuntimeConfig
import results
import libvvtest.testlistio as testlistio
import libvvtest.utesthooks as utesthooks
import libvvtest.resultswriter as resultswriter
import libvvtest.consolewriter as consolewriter
from libvvtest.outpututils import XstatusString, pretty_time, make_date_stamp
import libvvtest.pathutil as pathutil
from libvvtest.teststatus import TestStatusHandler, RESULTS_KEYWORDS
from libvvtest.printinfo import TestInformationPrinter


version = '1.2'

search_fnmatch = ['*.inp','*.apr','*.i']

testlist_name = 'testlist'


class MainEntry:

    def __init__(self, argv):
        ""
        self.argv = argv

    def constructObjects(self):
        ""
        start_time = time.time()

        check_for_bootstrap_file()

        # this may call sys.exit (for help or errors)
        self.opts, self.optD, self.dirs = \
            cmdline.parse_command_line( self.argv[1:], version )

        self.rtdata = RuntimeData( self.argv, get_tools_directory() )

        self.constructConfiguration()

        insert_configdir_into_sys_path( self.rtdata )

        test_cache_dir = self.readCacheData()

        self.constructPlatform()

        self.constructTestSubdirectory()

        self.rtdata.setTestResultsDir( test_cache_dir )

        self.rtdata.setPermissionsObject( self.opts, self.optD )

        self.rtdata.setTestStatusHandler( TestStatusHandler() )

        self.constructResultsWriter( start_time )

        self.rtdata.setFilterPath()

        self.constructRuntimeConfig()

    def execute(self):
        ""
        if self.opts.dash_i or self.opts.keys or self.opts.files:
            mode = InformationMode( self.opts, self.optD, self.rtdata )
            mode.run( self.dirs )

        elif self.opts.dash_g:
            generateTestList( self.opts, self.optD, self.dirs, self.rtdata )

        elif self.opts.dash_b:

            if self.opts.dash_R or self.opts.dash_w:
                print3( "*** error: cannot use -R or -w with -b (baseline)" )
                sys.exit(1)

            baselineTests( self.opts, self.optD, self.rtdata )

        elif self.opts.extract:
            extractTestFiles( self.opts, self.optD, self.dirs, self.opts.extract, self.rtdata )

        else:

            # if no results keywords are specified, then add -k notrun/notdone
            if not self.opts.dash_w and not self.opts.dash_R:
                rtconfig = self.rtdata.getRuntimeConfig()
                rtconfig.addResultsKeywordExpression( 'notrun or notdone' )

            if self.rtdata.isRestartMode():
                restartTests( self.opts, self.optD, self.rtdata )
            else:
                runTests( self.opts, self.optD, self.rtdata, self.dirs )

    def constructConfiguration(self):
        ""
        config = create_configuration( self.opts, self.optD,
                                       self.rtdata.getToolsDir() )
        self.rtdata.setConfiguration( config )

    def readCacheData(self):
        ""
        # non-None only if the CWD is in a TestResults.* directory
        test_cache_dir = readCommandInfo( self.opts, self.optD, self.rtdata )

        if test_cache_dir and self.optD['param_dict']:
            print3( "*** error: cannot use -S in a TestResults directory" )
            sys.exit(1)

        if test_cache_dir and self.opts.dash_g:
            print3( "*** error: cannot use -g in a TestResults directory" )
            sys.exit(1)

        # this is scheduled for deprecation
        if self.opts.check:
            for n in self.opts.check:
                os.environ[ 'CHECK_' + n.upper() ] = ''

        return test_cache_dir

    def constructPlatform(self):
        ""
        plat = construct_platform_instance( self.rtdata.getToolsDir(),
                                            self.opts, self.optD )
        self.rtdata.setPlatformObject( plat )

    def constructTestSubdirectory(self):
        ""
        sd = test_results_subdir_name( self.opts.run_dir,
                                       self.optD['onopts'], self.optD['offopts'],
                                       self.rtdata.getPlatformObject().getName() )
        self.rtdata.setTestSubdir( sd )

    def constructResultsWriter(self, start_time):
        ""
        resw = make_results_writer( self.opts, self.optD, start_time,
                                    self.rtdata.getToolsDir(),
                                    self.rtdata.getTestResultsDir(),
                                    self.rtdata.getPermissionsObject(),
                                    self.rtdata.getPlatformObject(),
                                    self.rtdata.cmdL,
                                    self.rtdata.getTestStatusHandler() )
        self.rtdata.setResultsWriter( resw )

    def constructRuntimeConfig(self):
        ""
        rtconfig = construct_RuntimeConfig( self.rtdata.getPlatformObject(),
                                            self.opts, self.optD )
        self.rtdata.setRuntimeConfig( rtconfig )


class Configuration:
    
    defaults = { \
                 'toolsdir':None,  # the top level tools directory
                 'configdir':None,  # the configuration directory
                 'exepath':None,  # the path to the executables
                 'onopts':[],
                 'offopts':[],
                 'refresh':1,
                 'postclean':0,
                 'timeout':None,
                 'multiplier':1.0,
                 'preclean':1,
                 'analyze':0,
                 'logfile':1,
                 'testargs':[],
               }
    
    def get(self, name):
        ""
        return self.attrs[name]
    
    def set(self, name, value):
        ""
        if value == None:
          self.attrs[name] = Configuration.defaults[name]
        else:
          self.attrs[name] = value
    
    def __init__(self):
        self.attrs = {}
        for n,v in Configuration.defaults.items():
          self.attrs[n] = v


def create_configuration( opts, optD, toolsdir ):
    ""
    config = Configuration()

    config.set( 'toolsdir', toolsdir )

    if optD['onopts']:
        config.set( 'onopts', optD['onopts'] )
    if optD['offopts']:
        config.set( 'offopts', optD['offopts'] )

    if opts.bin_dir:
        config.set( 'exepath', opts.bin_dir )

    if opts.config:
        config.set( 'configdir', opts.config[-1] )
    else:
        d = os.getenv( 'VVTEST_CONFIGDIR' )
        if d == None:
            d = os.path.join( toolsdir, 'config' )
        config.set( 'configdir', os.path.abspath(d) )

    config.set( 'refresh', not opts.dash_m )
    config.set( 'postclean', opts.postclean == True )

    if opts.dash_T != None:
        config.set( 'timeout', opts.dash_T )
    if opts.timeout_multiplier != None:
        config.set( 'multiplier', opts.timeout_multiplier )

    config.set( 'preclean', not opts.dash_m )
    config.set( 'analyze', opts.analyze == True )
    config.set( 'logfile', not opts.dash_L )

    if opts.test_args:
        argL = []
        for args in opts.test_args:
            argL.extend( shlex.split( args ) )
        config.set( 'testargs', argL )

    return config


class RuntimeData:

    def __init__(self, cmdL, toolsdir):
        """
        The 'toolsdir' is the directory containing this script.
        """
        self.cmdL = [ os.path.abspath( cmdL[0] ) ]
        self.cmdL.extend( cmdL[1:] )

        self.toolsdir = toolsdir

    def getToolsDir(self): return self.toolsdir

    def setConfiguration(self, config): self.config = config
    def getConfiguration(self): return self.config

    def setPlatformObject(self, platobj): self.plat = platobj
    def getPlatformObject(self): return self.plat

    def setRuntimeConfig(self, rtconfig): self.rtconfig = rtconfig
    def getRuntimeConfig(self): return self.rtconfig

    def setTestSubdir(self, subdir): self.testsubdir = subdir
    def getTestSubdir(self): return self.testsubdir

    def setTestResultsDir(self, test_cache_dir):
        ""
        if test_cache_dir:
            assert os.path.isabs( test_cache_dir )
            self.test_dir = test_cache_dir
            self.is_restart = True
        else:
            self.test_dir = os.path.abspath( self.testsubdir )
            self.is_restart = False

    def getTestResultsDir(self): return self.test_dir

    def isRestartMode(self):
        """
        True if the CWD is within an existing test results directory
        """
        return self.is_restart

    def setPermissionsObject(self, opts, optD):
        self.perms = make_PermissionSetter( self.test_dir, opts.perms )
    def getPermissionsObject(self): return self.perms

    def setTestStatusHandler(self, statushandler):
        self.statushandler = statushandler
    def getTestStatusHandler(self): return self.statushandler

    def setResultsWriter(self, writer): self.results_writer = writer
    def getResultsWriter(self): return self.results_writer

    def setFilterPath(self):
        ""
        cwd = os.getcwd()
        if pathutil.issubdir( self.test_dir, cwd ):
            d = pathutil.compute_relative_path( self.test_dir, cwd )
            self.filterdir = d
        else:
            self.filterdir = None

    def getFilterPath(self):
        """
        If the current working directory is a subdir of an existing test
        results directory, then this returns the relative path from the
        top of the test results directory to the current working directory.
        """
        return self.filterdir


def construct_platform_instance( toolsdir, opts, optD ):
    ""
    plat = vvplatform.create_Platform_instance(
                toolsdir,
                opts.plat,
                optD['platopt_dict'],  # --platopt
                opts.dash_e,
                opts.dash_n,
                opts.dash_N,
                optD['onopts'],        # -o
                optD['offopts'],       # -O
                opts.qsub_id )         # --qsub-id

    return plat


def construct_RuntimeConfig( plat, opts, optD ):
    ""
    rtconfig = RuntimeConfig( \
                  param_expr_list=optD['param_list'],
                  keyword_expr=optD['keyword_expr'],
                  option_list=( optD['onopts'] + [plat.getCompiler()] ),
                  platform_name=plat.getName(),
                  ignore_platforms=( opts.dash_A == True ),
                  set_platform_expr=optD['platform_expr'],
                  search_file_globs=search_fnmatch,
                  search_regexes=optD['search_regexes'],
                  include_tdd=( opts.include_tdd == True ),
                  runtime_range=[ opts.tmin, opts.tmax ],
                  runtime_sum=opts.tsum,
                  maxprocs=plat.getMaxProcs() )

    if opts.qsub_id != None:
        rtconfig.setAttr( 'include_all', True )

    return rtconfig


class InformationMode:

    def __init__(self, opts, optD, rtdata):
        ""
        self.opts = opts
        self.optD = optD
        self.rtdata = rtdata

    def run(self, scan_dirs):
        ""
        # always include tdd in info mode
        self.rtdata.getRuntimeConfig().setAttr( 'include_tdd', True )

        plat = self.rtdata.getPlatformObject()

        tlist = self.loadTestList( scan_dirs )

        test_dir = self.rtdata.getTestResultsDir()

        if self.opts.keys:
            self.printKeywordUnion( tlist )

        elif self.opts.files:
            self.printTestFiles( tlist )

        else:
            self.rtdata.getResultsWriter().info( tlist )

    def loadTestList(self, scan_dirs):
        ""
        test_dir = self.rtdata.getTestResultsDir()

        tlist = self.createTestList()

        if self.rtdata.isRestartMode():
            tlist.readTestList()
            tlist.readTestResults()

        elif self.opts.keys or self.opts.files:
            scan_test_source_directories( tlist, scan_dirs,
                                          self.optD['param_dict'] )

        elif os.path.exists( test_dir ):
            tlist.readTestList()
            tlist.readTestResults()

        tlist.determineActiveTests( filter_dir=self.rtdata.getFilterPath() )

        return tlist

    def createTestList(self):
        ""
        rtconfig = self.rtdata.getRuntimeConfig()
        statushandler = self.rtdata.getTestStatusHandler()
        test_dir = self.rtdata.getTestResultsDir()

        tfile = os.path.join( test_dir, testlist_name )
        if not os.path.exists( tfile ):
            tfile = None

        return TestList.TestList( statushandler, tfile, rtconfig )

    def printKeywordUnion(self, tlist):
        ""
        print3( "\nresults keywords: " + ' '.join( RESULTS_KEYWORDS ) )
        kd = {}
        for t in tlist.getActiveTests():
            for k in t.getKeywords():
                if k not in RESULTS_KEYWORDS and k != t.getName():
                    kd[k] = None
        L = list( kd.keys() )
        L.sort()
        print3( "\ntest keywords: " )
        while len(L) > 0:
            k1 = L.pop(0)
            if len(L) > 0: k2 = L.pop(0)
            else:          k2 = ''
            if len(L) > 0: k3 = L.pop(0)
            else:          k3 = ''
            print3( "  %-20s %-20s %-20s" % (k1,k2,k3) )

    def printTestFiles(self, tlist):
        ""
        D = {}
        for t in tlist.getActiveTests():
            d = os.path.normpath( t.getFilename() )
            D[d] = None
        L = list( D.keys() )
        L.sort()
        for d in L:
            print3( d )


##############################################################################
#
# generation of tests


def scan_test_source_directories( tlist, scan_dirs, setparams ):
    ""
    # default scan directory is the current working directory
    if len(scan_dirs) == 0:
        scan_dirs = ['.']

    for d in scan_dirs:
        if not os.path.exists(d):
            sys.stderr.write(
                '*** error: directory does not exist: ' + str(d) + '\n')
            sys.exit(1);
        tlist.scanDirectory( d, setparams )


def generateTestList( opts, optD, dirs, rtdata ):
    """
    """
    rtconfig = rtdata.getRuntimeConfig()
    config = rtdata.getConfiguration()
    plat = rtdata.getPlatformObject()
    testsubdir = rtdata.getTestSubdir()
    perms = rtdata.getPermissionsObject()
    statushandler = rtdata.getTestStatusHandler()

    test_dir = os.path.abspath( testsubdir )

    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( statushandler, tfile, rtconfig )

    scan_test_source_directories( tlist, dirs, optD['param_dict'] )

    loadRunTimes( statushandler, tlist, plat,
                  opts.dash_T, opts.timeout_multiplier, opts.max_timeout )

    tlist.applyPermanentFilters()

    rtdata.getResultsWriter().prerun( tlist, abbreviate=False )
    print3()

    createTestDir( testsubdir, perms, opts.dash_M )
    writeCommandInfo( opts, optD, rtdata, test_dir, plat, perms )

    tlist.stringFileWrite()
    perms.set( os.path.abspath( tfile ) )

    tlist.createTestExecs( test_dir, plat, config, perms )
    
    print3( "Test directory:", testsubdir )


def extractTestFiles( opts, optD, dirs, target_dir, rtdata ):
    """
    Uses all the regular filtering mechanisms to gather tests from a test
    source area and copies the files used for each test into a separate
    directory.
    """
    plat = rtdata.getPlatformObject()
    rtconfig = rtdata.getRuntimeConfig()
    statushandler = rtdata.getTestStatusHandler()

    tlist = TestList.TestList( statushandler, None, rtconfig )

    scan_test_source_directories( tlist, dirs, optD['param_dict'] )

    loadRunTimes( statushandler, tlist, plat,
                  opts.dash_T, opts.timeout_multiplier, opts.max_timeout )

    tlist.applyPermanentFilters()

    if not os.path.isabs(target_dir):
        target_dir = os.path.abspath(target_dir)
    if not os.path.exists(target_dir):
        os.makedirs( target_dir )
    
    uniqD = {}
    
    def wvisit( arg, dname, dirs, files ):
        """
        copy a directory tree, but leave out version control files
        """
        for n in ['CVS','.cvsignore','.svn','.git','.gitignore']:
            while (n in dirs): dirs.remove(n)
            while (n in files): files.remove(n)
        fd = os.path.normpath( os.path.join( arg[0], dname ) )
        td = os.path.normpath( os.path.join( arg[1], dname ) )
        if not os.path.exists(td):
            os.makedirs(td)
        for f1 in files:
            f2 = os.path.join(fd,f1)
            tf = os.path.join(td,f1)
            shutil.copy2( f2, tf )
    
    for t in tlist.getActiveTests():

        tname = t.getName()
        T = (tname, t.getFilename())

        from_dir = os.path.dirname( t.getFilename() )
        p = os.path.dirname( t.getFilepath() )
        if p: to_dir = os.path.normpath( os.path.join( target_dir, p ) )
        else: to_dir = target_dir

        if not os.path.exists( to_dir ):
            os.makedirs( to_dir )
        tof = os.path.join( target_dir, t.getFilepath() )

        if tof not in uniqD:
            uniqD[tof] = None
            try: shutil.copy2( t.getFilename(), tof )
            except IOError: pass

        for srcf in t.getSourceFiles():

            if os.path.exists( os.path.join( from_dir, srcf ) ):
                fL = [ srcf ]
            else:
                cwd = os.getcwd()
                try:
                    os.chdir( from_dir )
                    fL = glob.glob( srcf )
                except Exception:
                    fL = []
                os.chdir( cwd )

            for f in fL:
                fromf = os.path.join( from_dir, f )
                tof = os.path.join( to_dir, f )
                tod = os.path.dirname(tof)
                if tof not in uniqD:
                    uniqD[tof] = None
                    if not os.path.exists(tod):
                        os.makedirs(tod)
                    
                    if os.path.isdir(fromf):
                        cwd = os.getcwd()
                        os.chdir(fromf)
                        for root,dirs,files in os.walk( '.' ):
                            wvisit( (fromf, tof), root, dirs, files )
                        os.chdir(cwd)
                      
                    else:
                        try: shutil.copy2( fromf, tof )
                        except IOError: pass


##############################################################################


def test_results_subdir_name( rundir, onopts, offopts, platform_name ):
    """
    Generates and returns the subdirectory name to hold test results, which is
    unique up to the platform and on/off options.
    """
    if rundir:
        testdirname = rundir

    else:
        testdirname = 'TestResults.' + platform_name
        if onopts and len(onopts) > 0:
          testdirname += '.ON=' + '_'.join( onopts )
        if offopts and len(offopts) > 0:
          testdirname += '.OFF=' + '_'.join( offopts )
    
    return testdirname


def createTestDir( testdirname, perms, mirdir ):
    """
    Create the given directory name.  If -M is given in the command line
    options, then a mirror directory is created and 'testdirname' will be
    created as a soft link pointing to the mirror directory.
    """
    if mirdir and makeMirrorDirectory( mirdir, testdirname, perms ):
        pass

    else:
        if os.path.exists( testdirname ):
            if not os.path.isdir( testdirname ):
                # replace regular file with a directory
                os.remove( testdirname )
                os.mkdir( testdirname )
        else:
            if os.path.islink( testdirname ):
                os.remove( testdirname )  # remove broken softlink
            os.mkdir( testdirname )

        perms.set( os.path.abspath( testdirname ) )


def makeMirrorDirectory( Mval, testdirname, perms ):
    """
    Create a directory in another location then soft link 'testdirname' to it.
    Returns False only if 'Mval' is the word "any" and a suitable scratch
    directory could not be found.
    """
    assert testdirname == os.path.basename( testdirname )

    if Mval == 'any':
      
        usr = getUserName()
        for d in ['/var/scratch', '/scratch', '/var/scratch1', '/scratch1', \
                  '/var/scratch2', '/scratch2', '/var/scrl1', '/gpfs1']:
            if os.path.exists(d) and os.path.isdir(d):
                ud = os.path.join( d, usr )
                if os.path.exists(ud):
                    if os.path.isdir(ud) and \
                       os.access( ud, os.X_OK ) and os.access( ud, os.W_OK ):
                        Mval = ud
                        break
                elif os.access( d, os.X_OK ) and os.access( d, os.W_OK ):
                    try:
                        os.mkdir(ud)
                    except Exception:
                        pass
                    else:
                        Mval = ud
                        break
        
        if Mval == 'any':
            return False  # a scratch dir could not be found
        
        # include the current directory name in the mirror location
        curdir = os.path.basename( os.getcwd() )
        Mval = os.path.join( Mval, curdir )

        if not os.path.exists( Mval ):
            os.mkdir( Mval )

    else:
        Mval = os.path.abspath( Mval )
    
    if not os.path.exists( Mval ) or not os.path.isdir( Mval ) or \
       not os.access( Mval, os.X_OK ) or not os.access( Mval, os.W_OK ):
        raise Exception( "invalid or non-existent mirror directory: "+Mval )

    if os.path.samefile( Mval, os.getcwd() ):
        raise Exception( "mirror directory and current working directory " + \
                "are the same: "+Mval+' == '+os.getcwd() )

    mirdir = os.path.join( Mval, testdirname )

    if os.path.exists( mirdir ):
        if not os.path.isdir( mirdir ):
            # replace regular file with a directory
            os.remove( mirdir )
            os.mkdir( mirdir )
    else:
        if os.path.islink( mirdir ):
            os.remove( mirdir )  # remove broken softlink
        os.mkdir( mirdir )
    
    perms.set( os.path.abspath( mirdir ) )

    if os.path.islink( testdirname ):
        path = os.readlink( testdirname )
        if path != mirdir:
            os.remove( testdirname )
            os.symlink( mirdir, testdirname )

    else:
        if os.path.exists( testdirname ):
            if os.path.isdir( testdirname ):
                shutil.rmtree( testdirname )
            else:
                os.remove( testdirname )
        os.symlink( mirdir, testdirname )
    
    return True


def writeCommandInfo( opts, optD, rtdata, test_dir, plat, perms ):
    """
    Creates the test results information file.
    """
    config = rtdata.getConfiguration()

    f = os.path.join(test_dir, 'test.cache')
    if not os.path.exists( f ):
        fp = open( f, "w" )
        fp.write( 'VERSION=' + str(version) + '\n' )
        fp.write( 'DIR=' + os.getcwd() + '\n' )
        if opts.plat:
              fp.write( 'PLATFORM=' + opts.plat.strip() + '\n' )
        else:
              fp.write( 'PLATFORM=' + plat.getName() + '\n' )
        if optD['param_list']:
            fp.write( 'PARAMETERS=' + str( optD['param_list'] ) + '\n' )
        if config.get('exepath'):
            fp.write( \
                'PROJECT=' + os.path.abspath( config.get('exepath') ) + '\n' )
        if optD['onopts']:
            fp.write( 'ONOPTS=' + '+'.join( optD['onopts'] ) + '\n' )
        if optD['offopts']:
            fp.write( 'OFFOPTS=' + '+'.join( optD['offopts'] ) + '\n' )
        if opts.dash_T != None:
            fp.write( 'TIMEOUT=' + str(opts.dash_T).strip() + '\n' )
        if opts.timeout_multiplier != None:
            fp.write( 'TIMEOUT_MULTIPLIER=' + \
                                   str(opts.timeout_multiplier).strip() + '\n' )
        if opts.dash_e:
            fp.write( 'USE_ENV=1\n' )
        if opts.dash_A:
            fp.write( 'ALL_PLATFORMS=1\n' )
        if opts.include_tdd:
            fp.write( 'INCLUDE_TDD=True\n' )
        if opts.check:
            fp.write( 'CHECK=' + ' '.join( opts.check ) + '\n' )
        fp.close()

    perms.set( os.path.abspath(f) )


def readCommandInfo( opts, optD, rtdata ):
    """
    Check for a file called 'test.cache' that indicates whether the
    current working directory is a TestResults directory (or subdirectory)
    then open that file for information.  The test results directory is
    returned, or None if not in a TestRestults directory.
    """
    config = rtdata.getConfiguration()

    # an environment variable is used to identify vvtest run recursion
    troot = os.environ.get( 'VVTEST_TEST_ROOT', None )

    test_cache = misc.find_vvtest_test_root_file(
                                        os.getcwd(), troot, 'test.cache' )

    if test_cache != None:

        if optD['onopts'] or optD['offopts'] or opts.dash_g:
            sys.stderr.write('*** error: ' + \
                'the -g, -o, and -O options are not allowed ' + \
                'in a TestResults directory\n')
            sys.exit(1);

        fp = open( test_cache, "r" )
        write_version = 0
        for line in fp.readlines():
            line = line.strip()
            kvpair = line.split( '=', 1 )
            if kvpair[0] == 'VERSION':
                write_version = kvpair[1]
            elif kvpair[0] == 'DIR':
                previous_run_dir = kvpair[1]
            elif kvpair[0] == 'PLATFORM':
                opts.plat = kvpair[1]
            elif kvpair[0] == 'PARAMETERS':
                L = eval( kvpair[1] )
                if optD['param_list']: optD['param_list'].extend(L)
                else:                  optD['param_list'] = L
            elif kvpair[0] == 'PROJECT':
                # do not replace if the command line contains -j
                if not opts.bin_dir:
                    opts.bin_dir = kvpair[1]
                    config.set( 'exepath', kvpair[1] )
            elif kvpair[0] == 'ONOPTS':
                optD['onopts'] = kvpair[1].split( '+' )
                config.set( 'onopts', optD['onopts'] )
            elif kvpair[0] == 'OFFOPTS':
                optD['offopts'] = kvpair[1].split( '+' )
                config.set( 'offopts', optD['offopts'] )
            elif kvpair[0] == 'TIMEOUT':
                # do not replace if the command line contains -T
                if opts.dash_T == None:
                    opts.dash_T = kvpair[1]
                    config.set( 'timeout', float(opts.dash_T) )
            elif kvpair[0] == 'TIMEOUT_MULTIPLIER':
                if not opts.timeout_multiplier:
                    opts.timeout_multiplier = float(kvpair[1])
                    config.set( 'multiplier', opts.timeout_multiplier )
            elif kvpair[0] == 'USE_ENV':
                opts.dash_e = True
            elif kvpair[0] == 'ALL_PLATFORMS':
                opts.dash_A = True
            elif kvpair[0] == 'INCLUDE_TDD':
                opts.include_tdd = True
            elif kvpair[0] == 'CHECK':
                opts.check = kvpair[1].split()
        fp.close()

    if test_cache != None:
        return os.path.dirname( test_cache )
    return None


def exec_path( testspec, test_dir ):
    ""
    xdir = testspec.getExecuteDirectory()
    return pathutil.relative_execute_directory( xdir, test_dir, os.getcwd() )


def runTests( opts, optD, rtdata, dirs ):
    """
    Executes a list of tests.
    """
    config = rtdata.getConfiguration()
    rtconfig = rtdata.getRuntimeConfig()
    plat = rtdata.getPlatformObject()
    testsubdir = rtdata.getTestSubdir()
    statushandler = rtdata.getTestStatusHandler()

    # determine the directory that stores the test results then create it
    test_dir = os.path.abspath( testsubdir )
    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( statushandler, tfile, rtconfig )

    check_for_currently_running_vvtest( tlist.getResultsFilenames(), opts.force )

    # this variable allows vvtest tests to run vvtest (ie, allows recursion)
    os.environ['VVTEST_TEST_ROOT'] = os.path.normpath( test_dir )

    perms = rtdata.getPermissionsObject()

    createTestDir( testsubdir, perms, opts.dash_M )

    if opts.dash_w:
        remove_directory_contents( testsubdir )

    writeCommandInfo( opts, optD, rtdata, test_dir, plat, perms )

    scan_test_source_directories( tlist, dirs, optD['param_dict'] )

    tlist.readTestList()

    loadRunTimes( statushandler, tlist, plat,
                  opts.dash_T, opts.timeout_multiplier, opts.max_timeout )

    tlist.applyPermanentFilters()

    # If there is a test curator, import it and use it. Test curators
    # can be used to enforce keyword conventions, set timeouts, etc.
    # After the curator is called, reload runtimes and re-apply filters.
    try:
        import vvtest_curator
    except ImportError:
        # Python 3.6+ will raise a ModuleNotFoundError that is a
        # sub-exception of ImportError which will be caught by this.
        #
        # To allow for vvtest to be a soft link to an installed vvtest area,
        # look for a curator file in the directory containing the soft link.
        bindir = os.path.dirname( os.path.abspath( sys.argv[0] ) )
        curate = os.path.join( bindir, 'vvtest_curator.py' )
        if os.path.exists( curate ):
            sys.path.append( bindir )
            import vvtest_curator
        else:
            vvtest_curator = None

    if (vvtest_curator is not None
        and hasattr(vvtest_curator, 'curateTestList')):
        vvtest_curator.curateTestList(tlist)

        loadRunTimes( statushandler, tlist, plat,
                      opts.dash_T, opts.timeout_multiplier, opts.max_timeout )

        tlist.applyPermanentFilters()

    # save the test list in the TestResults directory
    tlist.stringFileWrite()
    perms.set( os.path.abspath( tfile ) )

    tlist.readTestResults()
    tlist.ensureInlinedTestResultIncludes()

    tlist.determineActiveTests()

    results_writer = rtdata.getResultsWriter()

    if tlist.numActive() > 0:

        results_writer.prerun( tlist )
        print3()

        tlist.createTestExecs( test_dir, plat, config, perms )

        if not opts.batch:
            executeTestList( opts, optD, rtdata,
                             tlist, test_dir, plat, perms, tfile,
                             results_writer )

        else:
            batchTestList( opts, optD, rtdata,
                           tlist, test_dir, plat, perms,
                           results_writer )

        print3( "Test directory:", testsubdir )

    else:
        print3( "\n--------- no tests to run -----------\n" )

    results_writer.final( tlist )


def check_for_currently_running_vvtest( resultsfiles, optforce ):
    ""
    if not optforce:

        msg = '*** error: tests are currently running in another process\n' + \
              '    (or a previous run was killed); use --force to run anyway'

        if len(resultsfiles) > 0:

            rfile = resultsfiles[-1]

            tlr = testlistio.TestListReader( rfile )
            fin = tlr.scanForFinishDate()
            if fin == None:
                print3( msg )
                sys.exit(1)


def determine_verbose_integer( dash_v ):
    ""
    if dash_v:
        verb = 1 + dash_v
    else:
        verb = 1

    return verb


def make_results_writer( opts, optD,
                         start_time, toolsdir,
                         test_dir, perms,
                         platobj, cmdL, statushandler ):
    ""
    verb = determine_verbose_integer( opts.dash_v )

    conobj = consolewriter.ConsoleWriter( statushandler, sys.stdout, test_dir,
                                          verbose=verb )
    conobj.setSortingSpecification( optD['sort_letters'] )

    htmlobj = None
    junitobj = None
    gitlabobj = None
    wlistobj = None

    if opts.qsub_id == None:

        if opts.html:
            import libvvtest.htmlwriter as htmlwriter
            htmlobj = htmlwriter.HTMLWriter( statushandler,
                                             perms,
                                             opts.html,
                                             test_dir )

        if opts.junit:
            import libvvtest.junitwriter as junitwriter
            junitobj = junitwriter.JUnitWriter( statushandler,
                                                perms,
                                                opts.junit,
                                                test_dir )
            junitobj.setOutputDate( opts.results_date )

        if opts.gitlab:
            import libvvtest.gitlabwriter as gitlabwriter
            gitlabobj = gitlabwriter.GitLabWriter( statushandler,
                                                   perms,
                                                   opts.gitlab,
                                                   test_dir )
            gitlabobj.setSortingSpecification( optD['sort_letters'] )

        if opts.save_results:
            import libvvtest.listwriter as listwriter
            wlistobj = listwriter.ListWriter( statushandler,
                                              perms,
                                              platobj.testingDirectory(),
                                              test_dir )
            wlistobj.setOutputDate( opts.results_date )
            wlistobj.setNamingTags( optD['onopts'], opts.results_tag )

    writer = resultswriter.ResultsWriter( conobj, htmlobj, junitobj,
                                          gitlabobj, wlistobj )

    writer.setRunAttr( startdate=str(start_time)+' / '+time.ctime(start_time) )
    writer.setRunAttr( platform=platobj.getName() )
    writer.setRunAttr( compiler=platobj.getCompiler() )
    writer.setRunAttr( commandline=' '.join( cmdL ) )
    writer.setRunAttr( hostname=os.uname()[1] )
    writer.setRunAttr( rundir=test_dir )
    writer.setRunAttr( curdir=os.getcwd() )
    writer.setRunAttr( python=sys.executable )
    writer.setRunAttr( vvtest=toolsdir )
    writer.setRunAttr( PYTHONPATH=os.environ.get( 'PYTHONPATH', '' ) )
    writer.setRunAttr( PATH=os.environ.get( 'PATH', '' ) )
    writer.setRunAttr( LOADEDMODULES=os.environ.get( 'LOADEDMODULES', '' ) )

    return writer


def executeTestList( opts, optD, rtdata,
                     tlist, test_dir, plat, perms, tfile,
                     results_writer ):
    """
    """
    statushandler = rtdata.getTestStatusHandler()

    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )

    uthook = utesthooks.construct_unit_testing_hook( 'run', opts.qsub_id )

    rfile = tlist.initializeResultsFile()

    try:

        info = TestInformationPrinter( sys.stdout, tlist )

        # execute tests

        perms.set( os.path.abspath( rfile ) )

        cwd = os.getcwd()

        while True:

            tnext = tlist.popNext( plat )

            if tnext != None:
                print3( 'Starting:', exec_path( tnext.atest, test_dir ) )
                tnext.start()
                tlist.appendTestResult( tnext.atest )
            
            elif tlist.numRunning() == 0:
                break

            else:
                info.checkPrint()
                time.sleep(1)

            showprogress = False
            for tx in list( tlist.getRunning() ):
                if tx.poll():
                    xs = XstatusString( statushandler, tx, test_dir, cwd )
                    print3( "Finished:", xs )
                    tlist.testDone( tx )
                    showprogress = True
          
            uthook.check( tlist.numRunning(), tlist.numDone() )

            if showprogress:
                ndone = tlist.numDone()
                ntot = tlist.numActive()
                pct = 100 * float(ndone) / float(ntot)
                div = str(ndone)+'/'+str(ntot)
                dt = pretty_time( time.time() - starttime )
                print3( "Progress: " + div+" = %%%.1f"%pct + ', time = '+dt )

    finally:
        tlist.writeFinished()

    # any remaining tests cannot run, so print warnings
    tL = tlist.popRemaining()
    if len(tL) > 0:
        print3()
    for tx in tL:
        depset = tx.getDependencySet()
        deptx = depset.getBlocking()
        assert depset.numDependencies() > 0 and deptx != None
        if isinstance( deptx, TestExec.TestExec ):
            xdir = deptx.atest.getExecuteDirectory()
        else:
            xdir = deptx.getExecuteDirectory()
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to dependency "' + xdir + '"' )

    print3()
    results_writer.postrun( tlist )

    elapsed = pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def batchTestList( opts, optD, rtdata,
                   tlist, test_dir, plat, perms,
                   results_writer ):
    """
    The 'tlist' is a TestList class instance.
    """
    assert opts.qsub_id == None

    statushandler = rtdata.getTestStatusHandler()

    qsublimit = opts.batch_limit
    if qsublimit == None:
        qsublimit = plat.getDefaultQsubLimit()
    
    batch = Batcher( rtdata, opts, optD,
                     plat, test_dir, tlist, perms, qsublimit )
    
    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )

    results_suffix = tlist.setResultsSuffix()

    # write testlist files for each qsub
    numjobs = batch.writeQsubScripts( results_suffix )

    print3( 'Total number of batch jobs: ' + str(numjobs) + \
            ', maximum concurrent jobs: ' + str(qsublimit) )

    if opts.dash_g:
      return
    
    schedule = batch.getScheduler()

    cwd = os.getcwd()
    qsleep = int( os.environ.get( 'VVTEST_BATCH_SLEEP_LENGTH', 15 ) )

    uthook = utesthooks.construct_unit_testing_hook( 'batch' )

    rfile = tlist.initializeResultsFile()
    for inclf in batch.getIncludeFiles():
        tlist.addIncludeFile( inclf )

    info = TestInformationPrinter( sys.stdout, tlist, batch )

    try:
        while True:

            qid = schedule.checkstart()
            if qid != None:
                # nothing to print here because the qsubmit prints
                pass
            elif schedule.numInFlight() == 0:
                break
            else:
                sleep_with_info_check( info, qsleep )

            qidL,doneL = schedule.checkdone()
            
            if len(qidL) > 0:
                ids = ' '.join( [ str(qid) for qid in qidL ] )
                print3( 'Finished batch IDS:', ids )
            for t in doneL:
                ts = XstatusString( statushandler, t, test_dir, cwd )
                print3( "Finished:", ts )

            uthook.check( schedule.numInFlight(), schedule.numPastQueue() )

            if len(doneL) > 0:
                jpct = 100 * float(schedule.numDone()) / float(numjobs)
                jdiv = 'jobs '+str(schedule.numDone())+'/'+str(numjobs)
                jflt = '(in flight '+str(schedule.numStarted())+')'
                ndone = tlist.numDone()
                ntot = tlist.numActive()
                tpct = 100 * float(ndone) / float(ntot)
                tdiv = 'tests '+str(ndone)+'/'+str(ntot)
                dt = pretty_time( time.time() - starttime )
                print3( "Progress: " + \
                        jdiv+" = %%%.1f"%jpct + ' '+jflt+', ' + \
                        tdiv+" = %%%.1f"%tpct + ', ' + \
                        'time = '+dt )

        # any remaining tests cannot be run; flush then print warnings
        NS, NF, nrL = schedule.flush()

    finally:
        tlist.writeFinished()

    tlist.inlineIncludeFiles()

    perms.set( os.path.abspath( rfile ) )

    if len(NS)+len(NF)+len(nrL) > 0:
        print3()
    if len(NS) > 0:
      print3( "*** Warning: these batch numbers did not seem to start:",
              ' '.join(NS) )
    if len(NF) > 0:
      print3( "*** Warning: these batch numbers did not seem to finish:",
              ' '.join(NF) )
    for tx,deptx in nrL:
        depset = tx.getDependencySet()
        assert depset.numDependencies() > 0 and deptx != None
        if isinstance( deptx, TestExec.TestExec ):
            xdir = deptx.atest.getExecuteDirectory()
        else:
            xdir = deptx.getExecuteDirectory()
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to dependency "' + xdir + '"' )

    print3()
    results_writer.postrun( tlist )
    
    elapsed = pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def sleep_with_info_check( info, qsleep ):
    ""
    for i in range( int( qsleep + 0.5 ) ):
        info.checkPrint()
        time.sleep( 1 )


def loadRunTimes( statushandler, tlist, plat,
                  opttimeout, optmult, optmaxtimeout ):
    """
    For each test, a 'runtimes' file will be read (if it exists) and the
    run time for this platform extracted.  This run time is saved as the
    test execute time.  Also, a timeout is calculated for each test and
    placed in the 'timeout' attribute.
    """
    pname = plat.getName()
    cplr = plat.getCompiler()

    cache = results.LookupCache( pname, cplr, plat.testingDirectory() )

    for t in tlist.getTests():

        # grab explicit timeout value, if the test specifies it
        tout = t.getTimeout()

        # look for a previous runtime value
        tlen,tresult = cache.getRunTime( t )

        if tlen != None:

            rt = statushandler.getRuntime( t, None )
            if rt == None:
                statushandler.setRuntime( t, int(tlen) )

            if tout == None:
                if tresult == "timeout":
                    # for tests that timed out, make timeout much larger
                    if t.hasKeyword( "long" ):
                        # only long tests get timeouts longer than an hour
                        if tlen < 60*60:
                            tout = 4*60*60
                        elif tlen < 5*24*60*60:  # even longs are capped
                            tout = 4*tlen
                    else:
                        tout = 60*60

                else:
                    # pick timeout to allow for some runtime variability
                    if tlen < 120:
                        tout = max( 120, 2*tlen )
                    elif tlen < 300:
                        tout = max( 300, 1.5*tlen )
                    elif tlen < 4*60*60:
                        tout = int( float(tlen)*1.5 )
                    else:
                        tout = int( float(tlen)*1.3 )

        else:  # no previous result

            if tout != None:
                # use the explicit timeout value as the runtime
                tlen = tout
            else:
                # with no information, the default depends on 'long' keyword
                if t.hasKeyword("long"):
                    tlen = 5*60*60  # five hours
                else:
                    tlen = 60*60  # one hour
                tout = tlen

        if opttimeout != None:
            tout = int( float(opttimeout) )
        if optmult != None:
            tout = int( float(tout) * optmult )

        if optmaxtimeout != None:
            tout = min( tout, float(optmaxtimeout) )

        t.setAttr( 'timeout', tout )

    cache = None


class Batcher:
    
    def __init__(self, rtdata, opts, optD,
                       plat, test_dir, tlist, perms, maxjobs):
        """
        The 'tlist' is a TestList class instance.
        """
        self.rtdata = rtdata
        self.opts = opts
        self.optD = optD
        self.plat = plat
        self.test_dir = test_dir
        self.tlist = tlist
        self.perms = perms
        self.maxjobs = maxjobs
        self.clean_exit_marker = "queue job finished cleanly"

        # TODO: make Tzero a platform plugin thing
        self.Tzero = 21*60*60  # no timeout in batch mode is 21 hours

        # allow these values to be set by environment variable, mainly for
        # unit testing; if setting these is needed more regularly then a
        # command line option should be added
        val = int( os.environ.get( 'VVTEST_BATCH_READ_INTERVAL', 30 ) )
        self.read_interval = val
        val = int( os.environ.get( 'VVTEST_BATCH_READ_TIMEOUT', 5*60 ) )
        self.read_timeout = val

        self.qsub_testfilenames = []

        self.accountant = batchutils.BatchAccountant()
        self.namer = batchutils.BatchFileNamer( self.test_dir, testlist_name )

        self.createTestGroups()

        self.scheduler = batchutils.BatchScheduler(
                            self.test_dir, self.tlist,
                            self.rtdata.getTestStatusHandler(),
                            self.accountant, self.namer,
                            self.perms, self.plat, self.maxjobs,
                            self.clean_exit_marker )

    def getScheduler(self):
        return self.scheduler

    def getAccountant(self):
        return self.accountant

    def removeBatchDirectories(self):
        """
        """
        for d in self.namer.globBatchDirectories():
            print3( 'rm -rf '+d )
            fault_tolerant_remove( d )

    def createTestGroups(self):
        """
        """
        qlen = self.opts.batch_length
        if qlen == None:
            qlen = 30*60

        qL = []
        for np in self.tlist.getTestExecProcList():
          xL = []
          for tx in self.tlist.getTestExecList(np):
            xL.append( (tx.atest.getAttr('timeout'),tx) )
          xL.sort()
          grpL = []
          tsum = 0
          for rt,tx in xL:
            depset = tx.getDependencySet()
            if depset.numDependencies() > 0 or tx.atest.getAttr('timeout') < 1:
              # analyze tests and those with no timeout get their own group
              qL.append( [ self.Tzero, len(qL), [tx] ] )
            else:
              if len(grpL) > 0 and tsum + rt > qlen:
                qL.append( [ tsum, len(qL), grpL ] )
                grpL = []
                tsum = 0
              grpL.append( tx )
              tsum += rt
          if len(grpL) > 0:
            qL.append( [ tsum, len(qL), grpL ] )
        
        qL.sort()
        qL.reverse()
        self.qsublists = map( lambda L: L[2], qL )

    def writeQsubScripts(self, results_suffix):
        """
        """
        config = self.rtdata.getConfiguration()

        self.tlist.markTestsWithDependents()

        self.removeBatchDirectories()

        commonopts = ''
        if self.opts.dash_e: commonopts += ' -e'
        if self.opts.dash_m: commonopts += ' -m'
        if self.opts.postclean: commonopts += ' -C'
        if self.opts.analyze: commonopts += ' -a'
        if self.opts.dash_N: commonopts += ' -N '+str( self.opts.dash_N )
        if self.opts.perms:
            commonopts += ' --perms '+','.join( self.opts.perms )
        if config.get('configdir'):
            commonopts += ' --config='+config.get('configdir')
        if self.optD['platopt_dict']:
            for k,v in self.optD['platopt_dict'].items():
                if v:
                    commonopts += ' --platopt ' + k + '=' + v
                else:
                    commonopts += ' --platopt ' + k 
        for arg in config.get('testargs'):
            commonopts += ' --test-args="'+arg+'"'

        qsubids = {}  # maps batch id to max num processors for that batch
        
        qid = 0
        for qL in self.qsublists:
          self.make_queue_batch( qid, qL, qsubids, commonopts, results_suffix )
          qid += 1

        qidL = list( qsubids.keys() )
        qidL.sort()
        for i in qidL:
            incl = self.namer.getTestListName( i, relative=True )
            self.qsub_testfilenames.append( incl )

        for i in qidL:
            d = self.namer.getSubdir( i )
            self.perms.recurse( d )

        return len( qsubids )

    def getIncludeFiles(self):
        ""
        return self.qsub_testfilenames

    def make_queue_batch(self, qnumber, qlist, npD, comopts, results_suffix):
        """
        """
        qidstr = str(qnumber)

        testlistfname = self.namer.getTestListName( qidstr )
        statushandler = self.rtdata.getTestStatusHandler()

        tl = TestList.TestList( statushandler, testlistfname )
        tl.setResultsSuffix( results_suffix )

        tL = []
        maxnp = 0
        qtime = 0
        for tx in qlist:
          np = int( tx.atest.getParameters().get('np', 0) )
          if np <= 0: np = 1
          maxnp = max( maxnp, np )
          tl.addTest(tx.atest)
          tL.append( tx )
          qtime += int( tx.atest.getAttr('timeout') )
        
        if qtime == 0:
          qtime = self.Tzero  # give it the "no timeout" length of time
        else:
          # allow more time in the queue than calculated
          if qtime < 60:
            qtime = 120
          elif qtime < 600:
            qtime *= 2
          else:
            qtime = int( float(qtime) * 1.3 )

        if self.opts.max_timeout:
            qtime = min( qtime, float(self.opts.max_timeout) )

        npD[qnumber] = maxnp
        pout = self.namer.getBatchOutputName( qnumber )
        tout = self.namer.getTestListName( qnumber ) + '.' + results_suffix

        jb = batchutils.BatchJob( maxnp, pout, tout, tL,
                                  self.read_interval, self.read_timeout )
        self.accountant.addJob( qnumber, jb )
        
        tl.stringFileWrite( include_results_suffix=True )
        
        fn = self.namer.getBatchScriptName( qidstr )
        fp = open( fn, "w" )
        
        hdr = self.plat.getQsubScriptHeader( maxnp, qtime, self.test_dir, pout )
        fp.writelines( [ hdr + '\n\n',
                         'cd ' + self.test_dir + ' || exit 1\n',
                         'echo "job start time = `date`"\n' + \
                         'echo "job time limit = ' + str(qtime) + '"\n' ] )
        
        # set the environment variables from the platform into the script
        for k,v in self.plat.getEnvironment().items():
          fp.write( 'setenv ' + k + ' "' + v  + '"\n' )
        
        # collect relevant options to pass to the qsub vvtest invocation
        taopts = '--qsub-id=' + qidstr + ' '
        taopts += comopts
        if len(qlist) == 1:
          # force a timeout for batches with only one test
          if qtime < 600: taopts += ' -T ' + str(qtime*0.90)
          else:           taopts += ' -T ' + str(qtime-120)
        
        cmd = self.rtdata.getToolsDir()+'/vvtest ' + taopts + ' || exit 1'
        fp.writelines( [ cmd+'\n\n' ] )
        
        # echo a marker to determine when a clean batch job exit has occurred
        fp.writelines( [ 'echo "'+self.clean_exit_marker+'"\n' ] )
        
        fp.close()


def restartTests( opts, optD, rtdata ):
    ""
    config = rtdata.getConfiguration()
    rtconfig = rtdata.getRuntimeConfig()
    plat = rtdata.getPlatformObject()
    test_dir = rtdata.getTestResultsDir()
    statushandler = rtdata.getTestStatusHandler()

    # this variable allows vvtest tests to run vvtest (ie, allows recursion)
    os.environ['VVTEST_TEST_ROOT'] = test_dir

    qid = opts.qsub_id
    if qid == None:
        tfile = os.path.join( test_dir, testlist_name )
    else:
        # batch jobs have --qsub-id set and land here
        namer = batchutils.BatchFileNamer( test_dir, testlist_name )
        tfile = namer.getTestListName( qid )
        # prevent the run scripts from being written again
        config.set( 'refresh', False )

    tlist = TestList.TestList( statushandler, tfile, rtconfig )

    tlist.readTestList()
    tlist.readTestResults()
    tlist.ensureInlinedTestResultIncludes()

    check_for_currently_running_vvtest( tlist.getResultsFilenames(), opts.force )

    if qid == None:
        loadRunTimes( statushandler, tlist, plat,
                      opts.dash_T, opts.timeout_multiplier, opts.max_timeout )

    reld = rtdata.getFilterPath()

    tlist.determineActiveTests( filter_dir=reld )

    perms = rtdata.getPermissionsObject()

    perms.set( os.path.abspath( tfile ) )

    results_writer = rtdata.getResultsWriter()

    if tlist.numActive() > 0:

        results_writer.prerun( tlist )
        print3()

        tlist.createTestExecs( test_dir, plat, config, perms )

        if not opts.batch:
            executeTestList( opts, optD, rtdata,
                             tlist, test_dir, plat, perms, tfile,
                             results_writer )

        else:
            batchTestList( opts, optD, rtdata,
                           tlist, test_dir, plat, perms,
                           results_writer )

    else:
        print3( "\n--------- no tests to run -----------\n" )

    results_writer.final( tlist )


def baselineTests( opts, optD, rtdata ):
    ""
    rtconfig = rtdata.getRuntimeConfig()
    config = rtdata.getConfiguration()
    plat = rtdata.getPlatformObject()
    test_dir = rtdata.getTestResultsDir()
    statushandler = rtdata.getTestStatusHandler()

    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( statushandler, tfile, rtconfig )

    tlist.readTestList()
    tlist.readTestResults()
    tlist.ensureInlinedTestResultIncludes()

    # if the keyword expression does not include a results keyword, then
    # add the 'diff' keyword so that only diffs are rebaselined by default
    rtconfig.addResultsKeywordExpression( 'diff' )

    tlist.determineActiveTests( filter_dir=rtdata.getFilterPath(),
                                baseline=True )

    if tlist.numActive() > 0:

        perms = rtdata.getPermissionsObject()

        rtdata.getResultsWriter().prerun( tlist, abbreviate=False )
        print3()

        tlist.createTestExecs( test_dir, plat, config, perms )

        failures = False
        for tx in tlist.getTestExecList():

            if isinstance(tx, TestExec.TestExec):
                ref = tx.atest
            else:
                ref = tx

            sys.stdout.write( "baselining "+ref.getExecuteDirectory()+"..." )

            tx.start( baseline=1 )

            tm = int( os.environ.get( 'VVTEST_BASELINE_TIMEOUT', 30 ) )
            for i in range(tm):

                time.sleep(1)

                if tx.poll():
                    if statushandler.passed( tx.atest ):
                        print3( "done" )
                    else:
                        failures = True
                        print3("FAILED")
                    break

            if not tx.isDone():
                tx.killJob()
                failures = True
                print3( "TIMED OUT" )

        if failures:
          print3( "\n\n !!!!!!!!!!!  THERE WERE FAILURES  !!!!!!!!!! \n\n" )

    else:
        print3( "\n--------- no tests to baseline -----------\n" )


###########################################################################

def getUserName():
    """
    Retrieves the user name associated with this process.
    """
    usr = None
    try:
        import getpass
        usr = getpass.getuser()
    except Exception:
        usr = None
    
    if usr == None:
        try:
            uid = os.getuid()
            import pwd
            usr = pwd.getpwuid( uid )[0]
        except Exception:
            usr = None
    
    if usr == None:
        try:
            p = os.path.expanduser( '~' )
            if p != '~':
                usr = os.path.basename( p )
        except Exception:
            usr = None
    
    if usr == None:
        # try manually checking the environment
        for n in ['USER', 'LOGNAME', 'LNAME', 'USERNAME']:
            if os.environ.get(n,'').strip():
                usr = os.environ[n]
                break

    if usr == None:
        raise Exception( "could not determine this process's user name" )

    return usr


def print3( *args, **kwargs ):
    s = ' '.join( [ str(x) for x in args ] )
    if len(kwargs) > 0:
        s += ' ' + ' '.join( [ str(k)+'='+str(v) for k,v in kwargs.items() ] )
    sys.stdout.write( s + os.linesep )
    sys.stdout.flush()


###########################################################################

def get_tools_directory():
    ""
    d = sys.path[0]
    if not d:                  d = os.getcwd()
    elif not os.path.isabs(d): d = os.path.abspath(d)
    return d


def check_for_bootstrap_file():
    """
    if vvtest_bootstrap.py exists in the same directory as vvtest,
    then import it (which may set os.environ variables)
    """
    try:
        import vvtest_bootstrap

    except ImportError:
        # to allow for vvtest to be a soft link to an installed vvtest area,
        # look for a bootstrap file in the directory containing the soft link
        bindir = os.path.dirname( os.path.abspath( sys.argv[0] ) )
        boot = os.path.join( bindir, 'vvtest_bootstrap.py' )
        if os.path.exists( boot ):
            sys.path.append( bindir )
            import vvtest_bootstrap


def insert_configdir_into_sys_path( rtdata ):
    ""
    d1 = os.path.normpath( os.path.join( rtdata.getToolsDir(), 'config' ) )

    d2 = rtdata.getConfiguration().get( 'configdir' )
    if d2:
        d2 = os.path.normpath( d2 )

        if d1 != d2:
            sys.path.insert( 1, d1 )

        sys.path.insert( 1, d2 )

    else:
        sys.path.insert( 1, d1 )


def make_PermissionSetter( test_dir, optperms ):
    ""
    if optperms:
        from libvvtest.permsetter import PermissionSetter
        perms = PermissionSetter( test_dir, optperms )

    else:
        class DummyPermissionSetter:
            def __init__(self): pass
            def set(self, path): pass
            def recurse(self, path): pass

        perms = DummyPermissionSetter()

    return perms


def remove_directory_contents( path ):
    ""
    sys.stdout.write( 'rm -rf ' + path + '/* ...' )
    sys.stdout.flush()

    for f in os.listdir(path):
        df = os.path.join( path, f )
        fault_tolerant_remove( df )

    print3( 'done' )


def random_string( numchars=8 ):
    ""
    seq = string.ascii_uppercase + string.digits
    cL = [ random.choice( seq ) for _ in range(numchars) ]
    return ''.join( cL )


def fault_tolerant_remove( path, num_attempts=5 ):
    ""
    dn,fn = os.path.split( path )

    rmpath = os.path.join( dn, 'remove_'+fn + '_'+ random_string() )

    os.rename( path, rmpath )

    for i in range( num_attempts ):
        try:
            if os.path.islink( rmpath ):
                os.remove( rmpath )
            elif os.path.isdir( rmpath ):
                shutil.rmtree( rmpath )
            else:
                os.remove( rmpath )
            break
        except Exception:
            pass

        time.sleep(1)


##############################################################################

if __name__ == '__main__':
    ""
    main = MainEntry( sys.argv )
    main.constructObjects()
    main.execute()
