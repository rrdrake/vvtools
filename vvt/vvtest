#!/usr/bin/env python

# Copyright 2018 National Technology & Engineering Solutions of Sandia, LLC
# (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S.
# Government retains certain rights in this software.

import sys
sys.dont_write_bytecode = True
sys.excepthook = sys.__excepthook__
import os
import re
import stat
import time
import signal
import shutil
import types
import glob
import shlex
import string
import random

import libvvtest.cmdline as cmdline
import libvvtest.vvplatform as vvplatform
import libvvtest.misc as misc
import libvvtest.TestSpec as TestSpec
import libvvtest.TestExec as TestExec
import libvvtest.TestList as TestList
import libvvtest.batchutils as batchutils
import libvvtest.TestSpecCreator as TestSpecCreator
import libvvtest.FilterExpressions as FilterExpressions
from libvvtest.RuntimeConfig import RuntimeConfig
import results
from libvvtest.PermissionSetter import DummyPermissionSetter
from libvvtest.PermissionSetter import PermissionSetter
import libvvtest.testlistio as testlistio
import libvvtest.utesthooks as utesthooks
import libvvtest.resultswriter as resultswriter
import libvvtest.pathutil as pathutil


version = '1.1'

search_fnmatch = ['*.inp','*.apr','*.i']

testlist_name = 'testlist'

# a FilterExpressions.WordExpression() object filled with the command line
# -k and -K specifications
keyword_expr = None


def main():
    ""
    check_for_bootstrap_file()

    rtdata = RuntimeData( sys.argv )

    # this may call sys.exit (for help or errors)
    opts,optD,dirs = cmdline.parse_command_line( sys.argv[1:], version )

    global keyword_expr
    keyword_expr = optD['keyword_expr']

    populate_configuration_from_options( opts, optD, rtdata )

    insert_configdir_into_sys_path( rtdata )

    # non-None only if the CWD is in a TestResults.* directory
    test_cache_dir = readCommandInfo( opts, optD, rtdata )

    if opts.check:
        for n in opts.check:
            os.environ[ 'CHECK_' + n.upper() ] = ''

    if test_cache_dir and optD['param_dict']:
        print3( "*** error: cannot use -S in a TestResults directory" )
        sys.exit(1)

    if test_cache_dir and opts.dash_g:
        print3( "*** error: cannot use -g in a TestResults directory" )
        sys.exit(1)

    rtdata.setPlatformObject( opts, optD )

    rtdata.setTestSubdir( opts.run_dir, optD['onopts'], optD['offopts'] )

    rtdata.setTestResultsDir( test_cache_dir )

    rtdata.setPermissionsObject( opts, optD )

    rtdata.setResultsWriter( opts, optD )

    rtdata.setFilterPath()

    rtdata.setRuntimeConfig( opts, optD )

    if opts.dash_i or opts.keys or opts.files:
        print_info_mode( opts, optD, rtdata, dirs )

    elif opts.dash_g:
        generateTestList( opts, optD, dirs, rtdata )

    elif opts.dash_b:

        if opts.dash_R or opts.dash_w:
            print3( "*** error: cannot use -R or -w with -b (baseline)" )
            sys.exit(1)

        baselineTests( opts, optD, rtdata )

    elif opts.extract:
        extractTestFiles( opts, optD, dirs, opts.extract, rtdata )

    else:

        # if no results keywords are specified, then add -k notrun/notdone
        if not keyword_expr.containsResultsKeywords() and \
           not opts.dash_w and not opts.dash_R:
            keyword_expr.append( ['notrun/notdone'], 'and' )

        if rtdata.isRestartMode():
            restartTests( opts, optD, rtdata )
        else:
            runTests( opts, optD, rtdata, dirs )


class Configuration:
    
    defaults = { \
                 'toolsdir':None,  # the top level tools directory
                 'configdir':None,  # the configuration directory
                 'exepath':None,  # the path to the executables
                 'onopts':[],
                 'offopts':[],
                 'refresh':1,
                 'postclean':0,
                 'timeout':None,
                 'multiplier':1.0,
                 'preclean':1,
                 'analyze':0,
                 'logfile':1,
                 'testargs':[],
               }
    
    def get(self, name):
        ""
        return self.attrs[name]
    
    def set(self, name, value):
        ""
        if value == None:
          self.attrs[name] = Configuration.defaults[name]
        else:
          self.attrs[name] = value
    
    def __init__(self):
        self.attrs = {}
        for n,v in Configuration.defaults.items():
          self.attrs[n] = v


def populate_configuration_from_options( opts, optD, rtdata ):
    ""
    config = rtdata.getConfiguration()

    if optD['onopts']:
        config.set( 'onopts', optD['onopts'] )
    if optD['offopts']:
        config.set( 'offopts', optD['offopts'] )

    if opts.bin_dir:
        config.set( 'exepath', opts.bin_dir )

    if opts.config:
        config.set( 'configdir', opts.config[-1] )
    else:
        d = os.getenv( 'VVTEST_CONFIGDIR' )
        if d == None:
            d = os.path.join( rtdata.getToolsDir(), 'config' )
        config.set( 'configdir', os.path.abspath(d) )

    config.set( 'refresh', not opts.dash_m )
    config.set( 'postclean', opts.postclean == True )

    if opts.dash_T != None:
        config.set( 'timeout', opts.dash_T )
    if opts.timeout_multiplier != None:
        config.set( 'multiplier', opts.timeout_multiplier )

    config.set( 'preclean', not opts.dash_m )
    config.set( 'analyze', opts.analyze == True )
    config.set( 'logfile', not opts.dash_L )

    if opts.test_args:
        argL = []
        for args in opts.test_args:
            argL.extend( shlex.split( args ) )
        config.set( 'testargs', argL )


class RuntimeData:

    def __init__(self, cmdL):
        ""
        self.cmdL = [ os.path.abspath( cmdL[0] ) ]
        self.cmdL.extend( cmdL[1:] )

        # directory containing this script
        self.toolsdir = get_tools_directory()

        self.config = Configuration()
        self.config.set( 'toolsdir', self.toolsdir )

    def getToolsDir(self): return self.toolsdir
    def getConfigDir(self): return self.config.get( 'configdir' )
    def getConfiguration(self): return self.config

    def setPlatformObject(self, opts, optD):
        self.plat = construct_platform_instance( self.toolsdir, opts, optD )
    def getPlatformObject(self): return self.plat

    def setRuntimeConfig(self, opts, optD):
        self.rtconfig = construct_RuntimeConfig( self.plat, opts, optD )
    def getRuntimeConfig(self): return self.rtconfig

    def setTestSubdir(self, optrundir, onopts, offopts):
        ""
        sd = test_results_subdir_name( optrundir, onopts, offopts,
                                       self.plat.getName() )
        self.testsubdir = sd

    def getTestSubdir(self):
        return self.testsubdir

    def setTestResultsDir(self, test_cache_dir):
        ""
        if test_cache_dir:
            assert os.path.isabs( test_cache_dir )
            self.test_dir = test_cache_dir
            self.is_restart = True
        else:
            self.test_dir = os.path.abspath( self.testsubdir )
            self.is_restart = False

    def getTestResultsDir(self): return self.test_dir

    def isRestartMode(self):
        """
        True if the CWD is within an existing test results directory
        """
        return self.is_restart

    def setPermissionsObject(self, opts, optD):
        self.perms = make_PermissionSetter( self.test_dir, opts.perms )
    def getPermissionsObject(self): return self.perms

    def setResultsWriter(self, opts, optD):
        resw = make_results_writer( self.test_dir, self.perms, opts, optD )
        self.results_writer = resw
    def getResultsWriter(self): return self.results_writer

    def setFilterPath(self):
        ""
        cwd = os.getcwd()
        if pathutil.issubdir( self.test_dir, cwd ):
            d = pathutil.compute_relative_path( self.test_dir, cwd )
            self.filterdir = d
        else:
            self.filterdir = None

    def getFilterPath(self):
        """
        If the current working directory is a subdir of an existing test
        results directory, then this returns the relative path from the
        top of the test results directory to the current working directory.
        """
        return self.filterdir


def construct_platform_instance( toolsdir, opts, optD ):
    ""
    plat = vvplatform.create_Platform_instance(
                toolsdir,
                opts.plat,
                optD['platopt_dict'],  # --platopt
                opts.dash_e,
                opts.dash_n,
                opts.dash_N,
                optD['onopts'],        # -o
                optD['offopts'],       # -O
                opts.qsub_id )         # --qsub-id

    return plat


def construct_RuntimeConfig( plat, opts, optD ):
    ""
    rtconfig = RuntimeConfig( \
                  param_expr_list=optD['param_list'],
                  keyword_expr=keyword_expr,
                  option_list=( optD['onopts'] + [plat.getCompiler()] ),
                  platform_name=plat.getName(),
                  ignore_platforms=( opts.dash_A == True ),
                  set_platform_expr=optD['platform_expr'],
                  search_file_globs=search_fnmatch,
                  search_regexes=optD['search_regexes'],
                  include_tdd=( opts.include_tdd == True ),
                  runtime_range=[ opts.tmin, opts.tmax ],
                  runtime_sum=opts.tsum )

    if opts.qsub_id != None:
        rtconfig.setAttr( 'include_all', True )

    return rtconfig


def print_info_mode( opts, optD, rtdata, scan_dirs ):
    ""
    plat = rtdata.getPlatformObject()

    tlist = construct_info_test_list( opts, optD, rtdata, scan_dirs )

    test_dir = rtdata.getTestResultsDir()

    if opts.keys or opts.files:
        keywordInformation( tlist, opts.keys, opts.files )

    elif opts.save_results:
        saveResults( opts, optD, tlist, plat, test_dir )

    else:
        rtdata.getResultsWriter().info( tlist )


def construct_info_test_list( opts, optD, rtdata, scan_dirs ):
    ""
    rtconfig = rtdata.getRuntimeConfig()
    plat = rtdata.getPlatformObject()
    test_dir = rtdata.getTestResultsDir()

    # always include tdd in info mode
    rtconfig.setAttr( 'include_tdd', True )

    if rtdata.isRestartMode():
        tfile = os.path.join( test_dir, testlist_name )
        tlist = TestList.TestList( tfile, rtconfig )
        tlist.readTestList()
        tlist.readTestResults()
        tlist.loadAndFilter( plat.getMaxProcs(),
                             filter_dir=rtdata.getFilterPath() )

    elif os.path.exists( test_dir ):
        tfile = os.path.join( test_dir, testlist_name )
        tlist = TestList.TestList( tfile, rtconfig )
        tlist.readTestList()
        tlist.readTestResults()
        tlist.loadAndFilter( plat.getMaxProcs() )

    elif opts.keys or opts.files:
        tlist = TestList.TestList( None, rtconfig )
        scan_test_source_directories( tlist, scan_dirs, optD['param_dict'] )
        tlist.loadAndFilter( plat.getMaxProcs(), prune=True )

    else:
        print3( "*** warning: previous TestResults directory not found" )
        tlist = TestList.TestList( None, rtconfig )
        tlist.loadAndFilter( plat.getMaxProcs(), prune=True )

    return tlist


##############################################################################
#
# generation of tests


def scan_test_source_directories( tlist, scan_dirs, setparams ):
    ""
    # default scan directory is the current working directory
    if len(scan_dirs) == 0:
        scan_dirs = ['.']

    for d in scan_dirs:
        if not os.path.exists(d):
            sys.stderr.write(
                '*** error: directory does not exist: ' + str(d) + '\n')
            sys.exit(1);
        tlist.scanDirectory( d, setparams )


def generateTestList( opts, optD, dirs, rtdata ):
    """
    """
    rtconfig = rtdata.getRuntimeConfig()
    config = rtdata.getConfiguration()
    plat = rtdata.getPlatformObject()
    testsubdir = rtdata.getTestSubdir()

    test_dir = os.path.abspath( testsubdir )

    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( tfile, rtconfig )

    scan_test_source_directories( tlist, dirs, optD['param_dict'] )

    loadRunTimes( tlist, plat,
                  opts.dash_T, opts.timeout_multiplier, opts.max_timeout )

    tlist.loadAndFilter( plat.getMaxProcs() )
    
    perms = rtdata.getPermissionsObject()
    
    createTestDir( testsubdir, perms, opts.dash_M )
    writeCommandInfo( opts, optD, rtdata, test_dir, plat, perms )

    rtdata.getResultsWriter().prerun( tlist, short=False )

    tlist.createTestExecs( test_dir, plat, config, perms )

    tlist.stringFileWrite()

    perms.set( os.path.abspath( tfile ) )
    
    print3( "Test directory:", testsubdir )


def extractTestFiles( opts, optD, dirs, target_dir, rtdata ):
    """
    Uses all the regular filtering mechanisms to gather tests from a test
    source area and copies the files used for each test into a separate
    directory.
    """
    plat = rtdata.getPlatformObject()
    rtconfig = rtdata.getRuntimeConfig()

    tlist = TestList.TestList( None, rtconfig )

    scan_test_source_directories( tlist, dirs, optD['param_dict'] )

    loadRunTimes( tlist, plat,
                  opts.dash_T, opts.timeout_multiplier, opts.max_timeout )

    tlist.loadAndFilter( plat.getMaxProcs() )
    
    if not os.path.isabs(target_dir):
      target_dir = os.path.abspath(target_dir)
    if not os.path.exists(target_dir):
      os.makedirs( target_dir )
    
    uniqD = {}
    
    def wvisit( arg, dname, dirs, files ):
        """
        copy a directory tree, but leave out version control files
        """
        for n in ['CVS','.cvsignore','.svn','.git','.gitignore']:
            while (n in dirs): dirs.remove(n)
            while (n in files): files.remove(n)
        fd = os.path.normpath( os.path.join( arg[0], dname ) )
        td = os.path.normpath( os.path.join( arg[1], dname ) )
        if not os.path.exists(td):
            os.makedirs(td)
        for f1 in files:
            f2 = os.path.join(fd,f1)
            tf = os.path.join(td,f1)
            shutil.copy2( f2, tf )
    
    for xdir,t in tlist.active.items():
      
      tname = t.getName()
      T = (tname, t.getFilename())
      
      from_dir = os.path.dirname( t.getFilename() )
      p = os.path.dirname( t.getFilepath() )
      if p: to_dir = os.path.normpath( os.path.join( target_dir, p ) )
      else: to_dir = target_dir
      
      if not os.path.exists( to_dir ):
        os.makedirs( to_dir )
      tof = os.path.join( target_dir, t.getFilepath() )
      
      if tof not in uniqD:
        uniqD[tof] = None
        try: shutil.copy2( t.getFilename(), tof )
        except IOError: pass
      
      for srcf in t.getSourceFiles():

        if os.path.exists( os.path.join( from_dir, srcf ) ):
            fL = [ srcf ]
        else:
            cwd = os.getcwd()
            try:
                os.chdir( from_dir )
                fL = glob.glob( srcf )
            except Exception:
                fL = []
            os.chdir( cwd )

        for f in fL:
            fromf = os.path.join( from_dir, f )
            tof = os.path.join( to_dir, f )
            tod = os.path.dirname(tof)
            if tof not in uniqD:
              uniqD[tof] = None
              if not os.path.exists(tod):
                os.makedirs(tod)
              
              if os.path.isdir(fromf):
                cwd = os.getcwd()
                os.chdir(fromf)
                for root,dirs,files in os.walk( '.' ):
                    wvisit( (fromf, tof), root, dirs, files )
                os.chdir(cwd)
                
              else:
                try: shutil.copy2( fromf, tof )
                except IOError: pass


##############################################################################

def keywordInformation( tlist, optkeys, optfiles ):
    
    if optkeys:
      print3( "\nresults keywords: " + ' '.join( TestSpec.results_keywords ) )
      kd = {}
      for t in tlist.getActiveTests():
        for k in t.getKeywords():
          if k not in TestSpec.results_keywords and k != t.getName():
            kd[k] = None
      L = list( kd.keys() )
      L.sort()
      print3( "\ntest keywords: " )
      while len(L) > 0:
        k1 = L.pop(0)
        if len(L) > 0: k2 = L.pop(0)
        else:          k2 = ''
        if len(L) > 0: k3 = L.pop(0)
        else:          k3 = ''
        print3( "  %-20s %-20s %-20s" % (k1,k2,k3) )
    else:
      assert optfiles
      D = {}
      for t in tlist.getActiveTests():
        d = os.path.normpath( t.getFilename() )
        D[d] = None
      L = list( D.keys() )
      L.sort()
      for d in L:
        print3( d )

##############################################################################


def test_results_subdir_name( rundir, onopts, offopts, platform_name ):
    """
    Generates and returns the subdirectory name to hold test results, which is
    unique up to the platform and on/off options.
    """
    if rundir:
        testdirname = rundir

    else:
        testdirname = 'TestResults.' + platform_name
        if onopts and len(onopts) > 0:
          testdirname += '.ON=' + '_'.join( onopts )
        if offopts and len(offopts) > 0:
          testdirname += '.OFF=' + '_'.join( offopts )
    
    return testdirname


def createTestDir( testdirname, perms, mirdir ):
    """
    Create the given directory name.  If -M is given in the command line
    options, then a mirror directory is created and 'testdirname' will be
    created as a soft link pointing to the mirror directory.
    """
    if mirdir and makeMirrorDirectory( mirdir, testdirname, perms ):
        pass

    else:
        if os.path.exists( testdirname ):
            if not os.path.isdir( testdirname ):
                # replace regular file with a directory
                os.remove( testdirname )
                os.mkdir( testdirname )
        else:
            if os.path.islink( testdirname ):
                os.remove( testdirname )  # remove broken softlink
            os.mkdir( testdirname )

        perms.set( os.path.abspath( testdirname ) )


def makeMirrorDirectory( Mval, testdirname, perms ):
    """
    Create a directory in another location then soft link 'testdirname' to it.
    Returns False only if 'Mval' is the word "any" and a suitable scratch
    directory could not be found.
    """
    assert testdirname == os.path.basename( testdirname )

    if Mval == 'any':
      
        usr = getUserName()
        for d in ['/var/scratch', '/scratch', '/var/scratch1', '/scratch1', \
                  '/var/scratch2', '/scratch2', '/var/scrl1', '/gpfs1']:
            if os.path.exists(d) and os.path.isdir(d):
                ud = os.path.join( d, usr )
                if os.path.exists(ud):
                    if os.path.isdir(ud) and \
                       os.access( ud, os.X_OK ) and os.access( ud, os.W_OK ):
                        Mval = ud
                        break
                elif os.access( d, os.X_OK ) and os.access( d, os.W_OK ):
                    try:
                        os.mkdir(ud)
                    except Exception:
                        pass
                    else:
                        Mval = ud
                        break
        
        if Mval == 'any':
            return False  # a scratch dir could not be found
        
        # include the current directory name in the mirror location
        curdir = os.path.basename( os.getcwd() )
        Mval = os.path.join( Mval, curdir )

        if not os.path.exists( Mval ):
            os.mkdir( Mval )

    else:
        Mval = os.path.abspath( Mval )
    
    if not os.path.exists( Mval ) or not os.path.isdir( Mval ) or \
       not os.access( Mval, os.X_OK ) or not os.access( Mval, os.W_OK ):
        raise Exception( "invalid or non-existent mirror directory: "+Mval )

    if os.path.samefile( Mval, os.getcwd() ):
        raise Exception( "mirror directory and current working directory " + \
                "are the same: "+Mval+' == '+os.getcwd() )

    mirdir = os.path.join( Mval, testdirname )

    if os.path.exists( mirdir ):
        if not os.path.isdir( mirdir ):
            # replace regular file with a directory
            os.remove( mirdir )
            os.mkdir( mirdir )
    else:
        if os.path.islink( mirdir ):
            os.remove( mirdir )  # remove broken softlink
        os.mkdir( mirdir )
    
    perms.set( os.path.abspath( mirdir ) )

    if os.path.islink( testdirname ):
        path = os.readlink( testdirname )
        if path != mirdir:
            os.remove( testdirname )
            os.symlink( mirdir, testdirname )

    else:
        if os.path.exists( testdirname ):
            if os.path.isdir( testdirname ):
                shutil.rmtree( testdirname )
            else:
                os.remove( testdirname )
        os.symlink( mirdir, testdirname )
    
    return True


def writeCommandInfo( opts, optD, rtdata, test_dir, plat, perms ):
    """
    Creates the test results information file.
    """
    config = rtdata.getConfiguration()

    f = os.path.join(test_dir, 'test.cache')
    if not os.path.exists( f ):
        fp = open( f, "w" )
        fp.write( 'VERSION=' + str(version) + '\n' )
        fp.write( 'DIR=' + os.getcwd() + '\n' )
        if opts.plat:
              fp.write( 'PLATFORM=' + opts.plat.strip() + '\n' )
        else:
              fp.write( 'PLATFORM=' + plat.getName() + '\n' )
        if optD['param_list']:
            fp.write( 'PARAMETERS=' + str( optD['param_list'] ) + '\n' )
        if config.get('exepath'):
            fp.write( \
                'PROJECT=' + os.path.abspath( config.get('exepath') ) + '\n' )
        if optD['onopts']:
            fp.write( 'ONOPTS=' + '+'.join( optD['onopts'] ) + '\n' )
        if optD['offopts']:
            fp.write( 'OFFOPTS=' + '+'.join( optD['offopts'] ) + '\n' )
        if opts.dash_T != None:
            fp.write( 'TIMEOUT=' + str(opts.dash_T).strip() + '\n' )
        if opts.timeout_multiplier != None:
            fp.write( 'TIMEOUT_MULTIPLIER=' + \
                                   str(opts.timeout_multiplier).strip() + '\n' )
        if opts.dash_e:
            fp.write( 'USE_ENV=1\n' )
        if opts.dash_A:
            fp.write( 'ALL_PLATFORMS=1\n' )
        if opts.include_tdd:
            fp.write( 'INCLUDE_TDD=True\n' )
        if opts.check:
            fp.write( 'CHECK=' + ' '.join( opts.check ) + '\n' )
        fp.close()

    perms.set( os.path.abspath(f) )


def readCommandInfo( opts, optD, rtdata ):
    """
    Check for a file called 'test.cache' that indicates whether the
    current working directory is a TestResults directory (or subdirectory)
    then open that file for information.  The test results directory is
    returned, or None if not in a TestRestults directory.
    """
    config = rtdata.getConfiguration()

    # an environment variable is used to identify vvtest run recursion
    troot = os.environ.get( 'VVTEST_TEST_ROOT', None )

    test_cache = misc.find_vvtest_test_root_file(
                                        os.getcwd(), troot, 'test.cache' )

    if test_cache != None:

        if optD['onopts'] or optD['offopts'] or opts.dash_g:
            sys.stderr.write('*** error: ' + \
                'the -g, -o, and -O options are not allowed ' + \
                'in a TestResults directory\n')
            sys.exit(1);

        fp = open( test_cache, "r" )
        write_version = 0
        for line in fp.readlines():
            line = line.strip()
            kvpair = line.split( '=', 1 )
            if kvpair[0] == 'VERSION':
                write_version = kvpair[1]
            elif kvpair[0] == 'DIR':
                previous_run_dir = kvpair[1]
            elif kvpair[0] == 'PLATFORM':
                opts.plat = kvpair[1]
            elif kvpair[0] == 'PARAMETERS':
                L = eval( kvpair[1] )
                if optD['param_list']: optD['param_list'].extend(L)
                else:                  optD['param_list'] = L
            elif kvpair[0] == 'PROJECT':
                # do not replace if the command line contains -j
                if not opts.bin_dir:
                    opts.bin_dir = kvpair[1]
                    config.set( 'exepath', kvpair[1] )
            elif kvpair[0] == 'ONOPTS':
                optD['onopts'] = kvpair[1].split( '+' )
                config.set( 'onopts', optD['onopts'] )
            elif kvpair[0] == 'OFFOPTS':
                optD['offopts'] = kvpair[1].split( '+' )
                config.set( 'offopts', optD['offopts'] )
            elif kvpair[0] == 'TIMEOUT':
                # do not replace if the command line contains -T
                if opts.dash_T == None:
                    opts.dash_T = kvpair[1]
                    config.set( 'timeout', float(opts.dash_T) )
            elif kvpair[0] == 'TIMEOUT_MULTIPLIER':
                if not opts.timeout_multiplier:
                    opts.timeout_multiplier = float(kvpair[1])
                    config.set( 'multiplier', opts.timeout_multiplier )
            elif kvpair[0] == 'USE_ENV':
                opts.dash_e = True
            elif kvpair[0] == 'ALL_PLATFORMS':
                opts.dash_A = True
            elif kvpair[0] == 'INCLUDE_TDD':
                opts.include_tdd = True
            elif kvpair[0] == 'CHECK':
                opts.check = kvpair[1].split()
        fp.close()

    if test_cache != None:
        return os.path.dirname( test_cache )
    return None


def load_filter_and_prune_test_list( test_dir, tlist, plat, filter_dir,
                                     optanalyze ):
    ""
    pruneL,cntmax = tlist.loadAndFilter( plat.getMaxProcs(),
                                         filter_dir=filter_dir,
                                         analyze_only=optanalyze,
                                         prune=True )
    
    if len(pruneL) > 0:
        print3()
        for pt,ct in pruneL:
            print3( '*** Warning: analyze test', exec_path( pt, test_dir ),
                    'will not be run due to dependency',
                    exec_path( ct, test_dir ) )
    if cntmax > 0:
        print3()
        print3( 'Note: there were', cntmax, 'tests that exceeded the',
                'maximum number of processors - they will not be run' )


def exec_path( testspec, test_dir ):
    ""
    xdir = testspec.getExecuteDirectory()
    return pathutil.relative_execute_directory( xdir, test_dir, os.getcwd() )


def runTests( opts, optD, rtdata, dirs ):
    """
    Executes a list of tests.
    """
    config = rtdata.getConfiguration()
    rtconfig = rtdata.getRuntimeConfig()
    plat = rtdata.getPlatformObject()
    testsubdir = rtdata.getTestSubdir()

    # determine the directory that stores the test results then create it
    test_dir = os.path.abspath( testsubdir )
    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( tfile, rtconfig )

    check_for_currently_running_vvtest( tlist.getResultsFilenames(), opts.force )

    # this variable allows vvtest tests to run vvtest (ie, allows recursion)
    os.environ['VVTEST_TEST_ROOT'] = os.path.normpath( test_dir )

    perms = rtdata.getPermissionsObject()

    createTestDir( testsubdir, perms, opts.dash_M )

    if opts.dash_w:
        remove_directory_contents( testsubdir )

    writeCommandInfo( opts, optD, rtdata, test_dir, plat, perms )

    scan_test_source_directories( tlist, dirs, optD['param_dict'] )

    # save the test list in the TestResults directory
    tlist.stringFileWrite()
    perms.set( os.path.abspath( tfile ) )

    tlist.readTestResults()
    tlist.ensureInlinedTestResultIncludes()

    loadRunTimes( tlist, plat,
                  opts.dash_T, opts.timeout_multiplier, opts.max_timeout )

    load_filter_and_prune_test_list( test_dir, tlist, plat, None, opts.analyze )

    results_writer = rtdata.getResultsWriter()

    if len(tlist.active) > 0:

        print3( "Running these tests:" )
        results_writer.prerun( tlist )
        print3()

        tlist.createTestExecs( test_dir, plat, config, perms )

        if opts.save_results:
            saveResults( opts, optD, tlist, plat, test_dir, inprogress=True )

        if not opts.batch:
            executeTestList( opts, optD,
                             tlist, test_dir, plat, perms, tfile,
                             results_writer )

        else:
            batchTestList( opts, optD, rtdata,
                           tlist, test_dir, plat, perms,
                           results_writer )

        print3( "Test directory:", testsubdir )

        if opts.save_results:
            saveResults( opts, optD, tlist, plat, test_dir )

    else:
        print3( "\n--------- no tests to run -----------\n" )

    results_writer.final( tlist )


def check_for_currently_running_vvtest( resultsfiles, optforce ):
    ""
    if not optforce:

        msg = '*** error: tests are currently running in another process\n' + \
              '    (or a previous run was killed); use --force to run anyway'

        if len(resultsfiles) > 0:

            rfile = resultsfiles[-1]

            tlr = testlistio.TestListReader( rfile )
            fin = tlr.scanForFinishDate()
            if fin == None:
                print3( msg )
                sys.exit(1)


def make_results_writer( test_dir, perms, opts, optD ):
    ""
    if opts.qsub_id == None:
        htmlfile = opts.html
        junitfile = opts.junit
        gitlabdir = opts.gitlab
    else:
        htmlfile = None
        junitfile = None
        gitlabdir = None

    writer = resultswriter.ResultsWriter(
                test_dir, perms,
                optD['sort_letters'], opts.results_date,
                htmlfile, junitfile, gitlabdir )

    return writer


def executeTestList( opts, optD,
                     tlist, test_dir, plat, perms, tfile,
                     results_writer ):
    """
    """
    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )

    uthook = utesthooks.construct_unit_testing_hook( 'run', opts.qsub_id )

    rfile = tlist.initializeResultsFile()

    try:

        # execute tests

        perms.set( os.path.abspath( rfile ) )

        cwd = os.getcwd()

        while True:

            tnext = tlist.popNext( plat )

            if tnext != None:
                print3( 'Starting:', exec_path( tnext.atest, test_dir ) )
                tnext.start()
                tlist.appendTestResult( tnext.atest )
            
            elif tlist.numRunning() == 0:
                break

            else:
                time.sleep(1)

            showprogress = False
            for tx in list( tlist.getRunning() ):
                if tx.poll():
                    xs = resultswriter.XstatusString( tx, test_dir, cwd )
                    print3( "Finished:", xs )
                    tlist.testDone( tx )
                    showprogress = True
          
            uthook.check( tlist.numRunning(), tlist.numDone() )

            if showprogress:
                ndone = tlist.numDone()
                ntot = tlist.numActive()
                pct = 100 * float(ndone) / float(ntot)
                div = str(ndone)+'/'+str(ntot)
                dt = resultswriter.pretty_time( time.time() - starttime )
                print3( "Progress: " + div+" = %%%.1f"%pct + ', time = '+dt )

    finally:
        tlist.writeFinished()

    # any remaining tests cannot run, so print warnings
    tL = tlist.popRemaining()
    if len(tL) > 0:
        print3()
    for tx in tL:
        deptx = tx.getBlockingDependency()
        assert tx.hasDependency() and deptx != None
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to dependency "' + deptx.atest.getExecuteDirectory() + '"' )

    print3()
    results_writer.postrun( tlist )

    elapsed = resultswriter.pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def batchTestList( opts, optD, rtdata,
                   tlist, test_dir, plat, perms,
                   results_writer ):
    """
    The 'tlist' is a TestList class instance.
    """
    assert opts.qsub_id == None

    qsublimit = opts.batch_limit
    if qsublimit == None:
        qsublimit = plat.getDefaultQsubLimit()
    
    batch = Batcher( rtdata, opts, optD,
                     plat, test_dir, tlist, perms, qsublimit )
    
    plat.display()
    starttime = time.time()
    print3( "Start time:", time.ctime() )

    results_suffix = tlist.setResultsSuffix()

    # write testlist files for each qsub
    numjobs = batch.writeQsubScripts( results_suffix )

    print3( 'Total number of batch jobs: ' + str(numjobs) + \
            ', maximum concurrent jobs: ' + str(qsublimit) )

    if opts.dash_g:
      return
    
    schedule = batch.getScheduler()

    cwd = os.getcwd()
    qsleep = int( os.environ.get( 'VVTEST_BATCH_SLEEP_LENGTH', 15 ) )

    uthook = utesthooks.construct_unit_testing_hook( 'batch' )

    rfile = tlist.initializeResultsFile()
    for inclf in batch.getIncludeFiles():
        tlist.addIncludeFile( inclf )

    try:
        while True:

            qid = schedule.checkstart()
            if qid != None:
                # nothing to print here because the qsubmit prints
                pass
            elif schedule.numInFlight() == 0:
                break
            else:
                time.sleep( qsleep )

            qidL,doneL = schedule.checkdone()
            
            if len(qidL) > 0:
                ids = ' '.join( [ str(qid) for qid in qidL ] )
                print3( 'Finished batch IDS:', ids )
            for t in doneL:
                ts = resultswriter.XstatusString( t, test_dir, cwd )
                print3( "Finished:", ts )

            uthook.check( schedule.numInFlight(), schedule.numPastQueue() )

            if len(doneL) > 0:
                jpct = 100 * float(schedule.numDone()) / float(numjobs)
                jdiv = 'jobs '+str(schedule.numDone())+'/'+str(numjobs)
                jflt = '(in flight '+str(schedule.numStarted())+')'
                ndone = tlist.numDone()
                ntot = tlist.numActive()
                tpct = 100 * float(ndone) / float(ntot)
                tdiv = 'tests '+str(ndone)+'/'+str(ntot)
                dt = resultswriter.pretty_time( time.time() - starttime )
                print3( "Progress: " + \
                        jdiv+" = %%%.1f"%jpct + ' '+jflt+', ' + \
                        tdiv+" = %%%.1f"%tpct + ', ' + \
                        'time = '+dt )

        # any remaining tests cannot be run; flush then print warnings
        NS, NF, nrL = schedule.flush()

    finally:
        tlist.writeFinished()

    tlist.inlineIncludeFiles()

    perms.set( os.path.abspath( rfile ) )

    if len(NS)+len(NF)+len(nrL) > 0:
        print3()
    if len(NS) > 0:
      print3( "*** Warning: these batch numbers did not seem to start:",
              ' '.join(NS) )
    if len(NF) > 0:
      print3( "*** Warning: these batch numbers did not seem to finish:",
              ' '.join(NF) )
    for tx,deptx in nrL:
        assert tx.hasDependency() and deptx != None
        print3( '*** Warning: analyze test skipped for "' + \
                tx.atest.getExecuteDirectory() + \
                '" due to dependency "' + deptx.atest.getExecuteDirectory() + '"' )

    print3()
    results_writer.postrun( tlist )
    
    elapsed = resultswriter.pretty_time( time.time() - starttime )
    print3( "\nFinish date:", time.ctime() + " (elapsed time "+elapsed+")" )


def loadRunTimes( tlist, plat, opttimeout, optmult, optmaxtimeout ):
    """
    For each test, a 'runtimes' file will be read (if it exists) and the
    run time for this platform extracted.  This run time is saved in the
    test as the 'runtime' attribute.  Also, a timeout is calculated for
    each test and placed in the 'timeout' attribute.
    """
    pname = plat.getName()
    cplr = plat.getCompiler()

    cache = results.LookupCache( pname, cplr, plat.testingDirectory() )

    for t in tlist.getTests():

        # grab explicit timeout value, if the test specifies it
        tout = t.getTimeout()

        # look for a previous runtime value
        tlen,tresult = cache.getRunTime( t )

        if tlen != None:

            t.setAttr( 'runtime', int(tlen) )

            if tout == None:
                if tresult == "timeout":
                    # for tests that timed out, make timeout much larger
                    if t.hasKeyword( "long" ):
                        # only long tests get timeouts longer than an hour
                        if tlen < 60*60:
                            tout = 4*60*60
                        elif tlen < 5*24*60*60:  # even longs are capped
                            tout = 4*tlen
                    else:
                        tout = 60*60

                else:
                    # pick timeout to allow for some runtime variability
                    if tlen < 120:
                        tout = max( 120, 2*tlen )
                    elif tlen < 300:
                        tout = max( 300, 1.5*tlen )
                    elif tlen < 4*60*60:
                        tout = int( float(tlen)*1.5 )
                    else:
                        tout = int( float(tlen)*1.3 )

        else:  # no previous result

            if tout != None:
                # use the explicit timeout value as the runtime
                tlen = tout
            else:
                # with no information, the default depends on 'long' keyword
                if t.hasKeyword("long"):
                    tlen = 5*60*60  # five hours
                else:
                    tlen = 60*60  # one hour
                tout = tlen

        if opttimeout != None:
            tout = int( float(opttimeout) )
        if optmult != None:
            tout = int( float(tout) * optmult )

        if optmaxtimeout != None:
            tout = min( tout, float(optmaxtimeout) )

        t.setAttr( 'timeout', tout )

    cache = None


class Batcher:
    
    def __init__(self, rtdata, opts, optD,
                       plat, test_dir, tlist, perms, maxjobs):
        """
        The 'tlist' is a TestList class instance.
        """
        self.rtdata = rtdata
        self.opts = opts
        self.optD = optD
        self.plat = plat
        self.test_dir = test_dir
        self.tlist = tlist
        self.perms = perms
        self.maxjobs = maxjobs
        self.clean_exit_marker = "queue job finished cleanly"

        # TODO: make Tzero a platform plugin thing
        self.Tzero = 21*60*60  # no timeout in batch mode is 21 hours

        # allow these values to be set by environment variable, mainly for
        # unit testing; if setting these is needed more regularly then a
        # command line option should be added
        val = int( os.environ.get( 'VVTEST_BATCH_READ_INTERVAL', 30 ) )
        self.read_interval = val
        val = int( os.environ.get( 'VVTEST_BATCH_READ_TIMEOUT', 5*60 ) )
        self.read_timeout = val

        self.qsub_testfilenames = []

        self.accountant = batchutils.BatchAccountant()
        self.namer = batchutils.BatchFileNamer( self.test_dir, testlist_name )

        self.createTestGroups()

        self.scheduler = batchutils.BatchScheduler(
                            self.test_dir, self.tlist,
                            self.accountant, self.namer,
                            self.perms, self.plat, self.maxjobs,
                            self.clean_exit_marker )

    def getScheduler(self):
        return self.scheduler

    def removeBatchDirectories(self):
        """
        """
        for d in self.namer.globBatchDirectories():
            print3( 'rm -rf '+d )
            fault_tolerant_remove( d )

    def createTestGroups(self):
        """
        """
        qlen = self.opts.batch_length
        if qlen == None:
            qlen = 30*60

        qL = []
        for np in self.tlist.getTestExecProcList():
          xL = []
          for tx in self.tlist.getTestExecList(np):
            xL.append( (tx.atest.getAttr('timeout'),tx) )
          xL.sort()
          grpL = []
          tsum = 0
          for rt,tx in xL:
            if tx.hasDependency() or tx.atest.getAttr('timeout') < 1:
              # analyze tests and those with no timeout get their own group
              qL.append( [ self.Tzero, len(qL), [tx] ] )
            else:
              if len(grpL) > 0 and tsum + rt > qlen:
                qL.append( [ tsum, len(qL), grpL ] )
                grpL = []
                tsum = 0
              grpL.append( tx )
              tsum += rt
          if len(grpL) > 0:
            qL.append( [ tsum, len(qL), grpL ] )
        
        qL.sort()
        qL.reverse()
        self.qsublists = map( lambda L: L[2], qL )

    def writeQsubScripts(self, results_suffix):
        """
        """
        config = self.rtdata.getConfiguration()

        self.tlist.markTestsWithDependents()

        self.removeBatchDirectories()

        commonopts = ''
        if self.opts.dash_e: commonopts += ' -e'
        if self.opts.dash_m: commonopts += ' -m'
        if self.opts.postclean: commonopts += ' -C'
        if self.opts.analyze: commonopts += ' -a'
        if self.opts.dash_N: commonopts += ' -N '+str( self.opts.dash_N )
        if self.optD['param_list']:
            # have to escape exclamation points
            for s in self.optD['param_list']:
                s = s.replace( '!', '\\!' )
                commonopts += ' -p "'+s+'"'
        if self.opts.perms:
            commonopts += ' --perms '+','.join( self.opts.perms )
        if config.get('configdir'):
            commonopts += ' --config='+config.get('configdir')
        if self.optD['platopt_dict']:
            for k,v in optD['platopt_dict'].items():
                if v:
                    commonopts += ' --platopt ' + k + '=' + v
                else:
                    commonopts += ' --platopt ' + k 
        for arg in config.get('testargs'):
            commonopts += ' --test-args="'+arg+'"'

        qsubids = {}  # maps batch id to max num processors for that batch
        
        qid = 0
        for qL in self.qsublists:
          self.make_queue_batch( qid, qL, qsubids, commonopts, results_suffix )
          qid += 1

        qidL = list( qsubids.keys() )
        qidL.sort()
        for i in qidL:
            incl = self.namer.getTestListName( i, relative=True )
            self.qsub_testfilenames.append( incl )

        for i in qidL:
            d = self.namer.getSubdir( i )
            self.perms.recurse( d )

        return len( qsubids )

    def getIncludeFiles(self):
        ""
        return self.qsub_testfilenames

    def make_queue_batch(self, qnumber, qlist, npD, comopts, results_suffix):
        """
        """
        qidstr = str(qnumber)

        testlistfname = self.namer.getTestListName( qidstr )

        tl = TestList.TestList( testlistfname )
        tl.setResultsSuffix( results_suffix )

        tL = []
        maxnp = 0
        qtime = 0
        for tx in qlist:
          np = int( tx.atest.getParameters().get('np', 0) )
          if np <= 0: np = 1
          maxnp = max( maxnp, np )
          tl.addTest(tx.atest)
          tL.append( tx )
          qtime += int( tx.atest.getAttr('timeout') )
        
        if qtime == 0:
          qtime = self.Tzero  # give it the "no timeout" length of time
        else:
          # allow more time in the queue than calculated
          if qtime < 60:
            qtime = 120
          elif qtime < 600:
            qtime *= 2
          else:
            qtime = int( float(qtime) * 1.3 )

        if self.opts.max_timeout:
            qtime = min( qtime, float(self.opts.max_timeout) )

        npD[qnumber] = maxnp
        pout = self.namer.getBatchOutputName( qnumber )
        tout = self.namer.getTestListName( qnumber ) + '.' + results_suffix

        jb = batchutils.BatchJob( maxnp, pout, tout, tL,
                                  self.read_interval, self.read_timeout )
        self.accountant.addJob( qnumber, jb )
        
        tl.stringFileWrite( include_results_suffix=True )
        
        fn = self.namer.getBatchScriptName( qidstr )
        fp = open( fn, "w" )
        
        hdr = self.plat.getQsubScriptHeader( maxnp, qtime, self.test_dir, pout )
        fp.writelines( [ hdr + '\n\n',
                         'cd ' + self.test_dir + ' || exit 1\n',
                         'echo "job start time = `date`"\n' + \
                         'echo "job time limit = ' + str(qtime) + '"\n' ] )
        
        # set the environment variables from the platform into the script
        for k,v in self.plat.getEnvironment().items():
          fp.write( 'setenv ' + k + ' "' + v  + '"\n' )
        
        # collect relevant options to pass to the qsub vvtest invocation
        taopts = '--qsub-id=' + qidstr + ' '
        taopts += comopts
        if len(qlist) == 1:
          # force a timeout for batches with only one test
          if qtime < 600: taopts += ' -T ' + str(qtime*0.90)
          else:           taopts += ' -T ' + str(qtime-120)
        
        cmd = self.rtdata.getToolsDir()+'/vvtest ' + taopts + ' || exit 1'
        fp.writelines( [ cmd+'\n\n' ] )
        
        # echo a marker to determine when a clean batch job exit has occurred
        fp.writelines( [ 'echo "'+self.clean_exit_marker+'"\n' ] )
        
        fp.close()


def saveResults( opts, optD, tlist, plat, test_dir, inprogress=False ):
    """
    Option is
    
      --save-results
    
    which writes to the platform config testing directory (which looks first at
    the TESTING_DIRECTORY env var).  Can add
    
      --results-tag <string>
    
    which is appended to the results file name.  A date string is embedded in
    the file name, which is obtained from the date of the first test that
    ran.  But if the option

      --results-date <float or string>

    is given on the vvtest command line, then that date is used instead.
    """
    pname = plat.getName()
    cplr = plat.getCompiler()

    rtag = opts.results_tag
    
    # determine the date to embed in the file name
    datestr = resultswriter.make_date_stamp(
                    tlist.getDateStamp( time.time() ), opts.results_date )

    L = []
    if optD['onopts']:
        for o in optD['onopts']:
            if o != cplr:
                L.append(o)
    L.sort()
    L.insert( 0, cplr )
    optstag = '+'.join(L)

    rdir = plat.testingDirectory()
    if rdir == None or not os.path.isdir(rdir):
      raise Exception( "invalid testing directory: " + str(rdir) )
    
    L = ['results',datestr,pname,optstag]
    if rtag != None: L.append(rtag)
    fname = os.path.join( rdir, '.'.join( L ) )
    
    tr = results.TestResults()
    for t in tlist.getActiveTests():
      tr.addTest(t)
    mach = os.uname()[1]
    tr.writeResults( fname, pname, cplr, mach, test_dir, inprogress )


def restartTests( opts, optD, rtdata ):
    ""
    config = rtdata.getConfiguration()
    rtconfig = rtdata.getRuntimeConfig()
    plat = rtdata.getPlatformObject()
    test_dir = rtdata.getTestResultsDir()

    # this variable allows vvtest tests to run vvtest (ie, allows recursion)
    os.environ['VVTEST_TEST_ROOT'] = test_dir

    qid = opts.qsub_id
    if qid == None:
        tfile = os.path.join( test_dir, testlist_name )
    else:
        # batch jobs have --qsub-id set and land here
        namer = batchutils.BatchFileNamer( test_dir, testlist_name )
        tfile = namer.getTestListName( qid )
        # prevent the run scripts from being written again
        config.set( 'refresh', False )

    tlist = TestList.TestList( tfile, rtconfig )

    tlist.readTestListIfNoTestResults()
    tlist.readTestResults()
    tlist.ensureInlinedTestResultIncludes()

    check_for_currently_running_vvtest( tlist.getResultsFilenames(), opts.force )

    if qid == None:
        loadRunTimes( tlist, plat,
                      opts.dash_T, opts.timeout_multiplier, opts.max_timeout )

    reld = rtdata.getFilterPath()

    load_filter_and_prune_test_list( test_dir, tlist, plat, reld, opts.analyze )

    perms = rtdata.getPermissionsObject()

    perms.set( os.path.abspath( tfile ) )

    results_writer = rtdata.getResultsWriter()

    if len(tlist.active) > 0:

        print3( "Running these tests:" )
        results_writer.prerun( tlist )
        print3()

        if opts.save_results:
            saveResults( opts, optD, tlist, plat, test_dir, inprogress=True )

        tlist.createTestExecs( test_dir, plat, config, perms )

        if not opts.batch:
            executeTestList( opts, optD,
                             tlist, test_dir, plat, perms, tfile,
                             results_writer )

        else:
            batchTestList( opts, optD, rtdata,
                           tlist, test_dir, plat, perms,
                           results_writer )

        if opts.save_results:
            saveResults( opts, optD, tlist, plat, test_dir )

    else:
        print3( "\n--------- no tests to run -----------\n" )

    results_writer.final( tlist )


def baselineTests( opts, optD, rtdata ):
    ""
    rtconfig = rtdata.getRuntimeConfig()
    config = rtdata.getConfiguration()
    plat = rtdata.getPlatformObject()
    test_dir = rtdata.getTestResultsDir()

    tfile = os.path.join( test_dir, testlist_name )

    tlist = TestList.TestList( tfile, rtconfig )

    tlist.readTestListIfNoTestResults()
    tlist.readTestResults()
    tlist.ensureInlinedTestResultIncludes()

    # if the keyword expression does not include a results keyword, then
    # add the 'diff' keyword so that only diffs are rebaselined by default
    if not keyword_expr.containsResultsKeywords():
        keyword_expr.append( ['diff'], 'and' )

    tlist.loadAndFilter( plat.getMaxProcs(),
                         filter_dir=rtdata.getFilterPath(),
                         baseline=True )

    if len(tlist.active) > 0:

        perms = rtdata.getPermissionsObject()

        print3( "Baselining these tests:" )
        rtdata.getResultsWriter().prerun( tlist, short=False )

        tlist.createTestExecs( test_dir, plat, config, perms )

        failures = False
        for tx in tlist.getTestExecList():

            if isinstance(tx, TestExec.TestExec):
                ref = tx.atest
            else:
                ref = tx

            sys.stdout.write( "baselining "+ref.getExecuteDirectory()+"..." )

            tx.start( baseline=1 )

            tm = int( os.environ.get( 'VVTEST_BASELINE_TIMEOUT', 30 ) )
            for i in range(tm):

                time.sleep(1)

                if tx.poll():
                    if tx.atest.getAttr('result') == "pass":
                        print3( "done" )
                    else:
                        failures = True
                        print3("FAILED")
                    break

            if not tx.isDone():
                tx.killJob()
                failures = True
                print3( "TIMED OUT" )

        if failures:
          print3( "\n\n !!!!!!!!!!!  THERE WERE FAILURES  !!!!!!!!!! \n\n" )

    else:
        print3( "\n--------- no tests to baseline -----------\n" )


###########################################################################

def getUserName():
    """
    Retrieves the user name associated with this process.
    """
    usr = None
    try:
        import getpass
        usr = getpass.getuser()
    except Exception:
        usr = None
    
    if usr == None:
        try:
            uid = os.getuid()
            import pwd
            usr = pwd.getpwuid( uid )[0]
        except Exception:
            usr = None
    
    if usr == None:
        try:
            p = os.path.expanduser( '~' )
            if p != '~':
                usr = os.path.basename( p )
        except Exception:
            usr = None
    
    if usr == None:
        # try manually checking the environment
        for n in ['USER', 'LOGNAME', 'LNAME', 'USERNAME']:
            if os.environ.get(n,'').strip():
                usr = os.environ[n]
                break

    if usr == None:
        raise Exception( "could not determine this process's user name" )

    return usr


def print3( *args, **kwargs ):
    s = ' '.join( [ str(x) for x in args ] )
    if len(kwargs) > 0:
        s += ' ' + ' '.join( [ str(k)+'='+str(v) for k,v in kwargs.items() ] )
    sys.stdout.write( s + os.linesep )
    sys.stdout.flush()


###########################################################################

def get_tools_directory():
    ""
    d = sys.path[0]
    if not d:                  d = os.getcwd()
    elif not os.path.isabs(d): d = os.path.abspath(d)
    return d


def check_for_bootstrap_file():
    """
    if vvtest_bootstrap.py exists in the same directory as vvtest,
    then import it (which may set os.environ variables)
    """
    try:
        import vvtest_bootstrap

    except ImportError:
        # to allow for vvtest to be a soft link to an installed vvtest area,
        # look for a bootstrap file in the directory containing the soft link
        bindir = os.path.dirname( os.path.abspath( sys.argv[0] ) )
        boot = os.path.join( bindir, 'vvtest_bootstrap.py' )
        if os.path.exists( boot ):
            sys.path.append( bindir )
            import vvtest_bootstrap


def insert_configdir_into_sys_path( rtdata ):
    ""
    d1 = os.path.normpath( os.path.join( rtdata.getToolsDir(), 'config' ) )

    d2 = rtdata.getConfigDir()
    if d2:
        d2 = os.path.normpath( d2 )

        if d1 != d2:
            sys.path.insert( 1, d1 )

        sys.path.insert( 1, d2 )

    else:
        sys.path.insert( 1, d1 )


def make_PermissionSetter( test_dir, optperms ):
    ""
    if optperms:
        perms = PermissionSetter( test_dir, optperms )
    else:
        perms = DummyPermissionSetter()

    return perms


def remove_directory_contents( path ):
    ""
    sys.stdout.write( 'rm -rf ' + path + '/* ...' )
    sys.stdout.flush()

    for f in os.listdir(path):
        df = os.path.join( path, f )
        fault_tolerant_remove( df )

    print3( 'done' )


def random_string( numchars=8 ):
    ""
    seq = string.ascii_uppercase + string.digits
    cL = [ random.choice( seq ) for _ in range(numchars) ]
    return ''.join( cL )


def fault_tolerant_remove( path, num_attempts=5 ):
    ""
    dn,fn = os.path.split( path )

    rmpath = os.path.join( dn, 'remove_'+fn + '_'+ random_string() )

    os.rename( path, rmpath )

    for i in range( num_attempts ):
        try:
            if os.path.islink( rmpath ):
                os.remove( rmpath )
            elif os.path.isdir( rmpath ):
                shutil.rmtree( rmpath )
            else:
                os.remove( rmpath )
            break
        except Exception:
            pass

        time.sleep(1)


##############################################################################

if __name__ == '__main__':

    main()
